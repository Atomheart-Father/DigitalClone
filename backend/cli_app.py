#!/usr/bin/env python3
"""
CLI Application for the Digital Clone AI Assistant.

Provides a command-line REPL interface with support for streaming output,
special commands, and AskUser state management.
"""

import sys
import argparse
import logging
from typing import List, Optional

# ç›´æ¥ä½¿ç”¨ç»å¯¹å¯¼å…¥ï¼ˆstart.pyå·²è®¾ç½®sys.pathï¼‰
import config
import message_types
import logger

# Import graph modules (start.pyå·²è®¾ç½®sys.path)
try:
    import graph
    import tool_prompt_builder
    graph_app = graph.graph_app
    planner_app = graph.planner_app
    create_initial_state = graph.state.create_initial_state
    load_system_prompt = tool_prompt_builder.load_system_prompt
    GRAPH_AVAILABLE = True
    logger.logger.info("Graph modules imported successfully")
except ImportError as e:
    logger.logger.warning(f"Graph modules not available: {e}")
    GRAPH_AVAILABLE = False
    load_system_prompt = None

# ä¸ºäº†å…¼å®¹æ€§ï¼Œåˆ›å»ºåˆ«å
config = config.config
Message = message_types.Message
Role = message_types.Role
ConversationLogger = logger.ConversationLogger

# Configure logging
logging.basicConfig(
    level=getattr(logging, config.LOG_LEVEL),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CLIApp:
    """Command-line interface for the AI assistant."""

    def __init__(self, stream: bool = False):
        self.stream = stream
        self.conversation_history: List[Message] = []
        self.awaiting_user_input = False
        self.logger = ConversationLogger()
        self.last_planner_state = None  # Save last planner state for continuation

    def run(self):
        """Main CLI application loop."""
        self._print_welcome()

        try:
            while True:
                if self.awaiting_user_input:
                    prompt = "è¯·æä¾›æ›´å¤šä¿¡æ¯> "
                else:
                    prompt = "> "

                try:
                    user_input = input(prompt).strip()
                except (EOFError, KeyboardInterrupt):
                    print("\nå†è§ï¼")
                    break

                if not user_input:
                    continue

                # Handle special commands
                if user_input.startswith(':'):
                    if self._handle_special_command(user_input):
                        break
                    continue

                # Process user input
                self._process_user_input(user_input)

        except Exception as e:
            logger.error(f"CLI application error: {e}")
            print(f"\nå‘ç”Ÿé”™è¯¯: {e}")
            print("è¯·æ£€æŸ¥æ—¥å¿—ä»¥è·å–è¯¦ç»†ä¿¡æ¯ã€‚")

    def _print_welcome(self):
        """Print welcome message and system information."""
        print("=" * 50)
        print("ğŸ­ èµ›åšå…‹éš† AI åŠ©æ‰‹")
        print("=" * 50)

        # Show model information
        if config.is_api_key_available():
            print(f"âœ“ å·²é…ç½® API å¯†é’¥")
            print(f"ğŸ“¡ Chatæ¨¡å‹: {config.MODEL_CHAT}")
            print(f"ğŸ¤– Reasoneræ¨¡å‹: {config.MODEL_REASONER}")
        else:
            print("âš ï¸  æœªé…ç½® API å¯†é’¥ï¼Œä½¿ç”¨ MockClient")

        # Show tool information
        from tool_registry import registry
        tools = registry.list_tools()
        print(f"ğŸ› ï¸  å·²åŠ è½½ {len(tools)} ä¸ªå·¥å…·: {[t.name for t in tools]}")

        print("\nè¾“å…¥æ‚¨çš„æ¶ˆæ¯ï¼Œæˆ–ä½¿ç”¨ç‰¹æ®Šå‘½ä»¤:")
        print("  :help  æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
        print("  :tools åˆ—å‡ºå¯ç”¨å·¥å…·")
        print("  :q     é€€å‡ºç¨‹åº")
        print("  :clear æ¸…ç©ºå¯¹è¯å†å²")
        print("  :graph æ˜¾ç¤ºå›¾çŠ¶æ€")
        print("  :route æ˜¾ç¤ºè·¯ç”±å†³ç­–")
        print("  :prompt [system|tools] æ˜¾ç¤ºç³»ç»Ÿæç¤º")
        print("  :plan  æ˜¾ç¤ºè®¡åˆ’è¯¦æƒ…")
        print("  :todo <id> æ˜¾ç¤ºç‰¹å®šTODOè¯¦æƒ…")
        print("-" * 50)

    def _handle_special_command(self, command: str) -> bool:
        """
        Handle special commands starting with ':'.

        Returns:
            True if should exit, False otherwise
        """
        cmd = command[1:].lower()

        if cmd == 'q' or cmd == 'quit':
            print("å†è§ï¼")
            return True

        elif cmd == 'help':
            self._show_help()
            return False

        elif cmd == 'tools':
            self._show_tools()
            return False

        elif cmd == 'clear':
            self.conversation_history.clear()
            self.awaiting_user_input = False
            print("âœ“ å·²æ¸…ç©ºå¯¹è¯å†å²")
            return False

        elif cmd == 'history':
            self._show_history()
            return False

        elif cmd == 'graph':
            self._show_graph_status()
            return False

        elif cmd == 'route':
            self._show_route_status()
            return False

        elif cmd == 'prompt':
            self._show_prompt_status(command)
            return False

        elif cmd == 'plan':
            self._show_plan_status()
            return False

        elif cmd.startswith('todo'):
            # :todo <id> - show specific todo details
            parts = cmd.split()
            if len(parts) == 2:
                todo_id = parts[1]
                self._show_todo_details(todo_id)
            else:
                print("ç”¨æ³•: :todo <id>")
                print("ç¤ºä¾‹: :todo T1")
            return False

        else:
            print(f"æœªçŸ¥å‘½ä»¤: {command}")
            print("è¾“å…¥ :help æŸ¥çœ‹å¯ç”¨å‘½ä»¤")
            return False

    def _show_help(self):
        """Show help information."""
        print("\nå¸®åŠ©ä¿¡æ¯:")
        print("  å¸¸è§„å¯¹è¯: ç›´æ¥è¾“å…¥æ‚¨çš„é—®é¢˜")
        print("  ç‰¹æ®Šå‘½ä»¤:")
        print("    :help   - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
        print("    :tools  - åˆ—å‡ºå¯ç”¨å·¥å…·")
        print("    :q      - é€€å‡ºç¨‹åº")
        print("    :clear  - æ¸…ç©ºå¯¹è¯å†å²")
        print("    :history- æ˜¾ç¤ºå¯¹è¯å†å²")
        print("\n  æµå¼è¾“å‡º: ä½¿ç”¨ --stream å‚æ•°å¯ç”¨")
        print("  AskUser: å½“åŠ©æ‰‹éœ€è¦æ›´å¤šä¿¡æ¯æ—¶ï¼Œä¼šæç¤ºæ‚¨æä¾›")

    def _show_tools(self):
        """Show available tools."""
        from tool_registry import registry
        tools = registry.list_tools()
        if not tools:
            print("æš‚æ— å¯ç”¨å·¥å…·")
            return

        print(f"\nå¯ç”¨å·¥å…· ({len(tools)} ä¸ª):")
        for tool in tools:
            print(f"  â€¢ {tool.name}: {tool.description}")
            # Show parameters if available
            if hasattr(tool, 'parameters') and tool.parameters.get('properties'):
                params = list(tool.parameters['properties'].keys())
                print(f"    å‚æ•°: {', '.join(params)}")
            # Show executor info
            if hasattr(tool, 'executor_default'):
                print(f"    æ‰§è¡Œè€…: {tool.executor_default}")
            if hasattr(tool, 'complexity'):
                print(f"    å¤æ‚åº¦: {tool.complexity}")

    def _show_history(self):
        """Show conversation history."""
        if not self.conversation_history:
            print("æš‚æ— å¯¹è¯å†å²")
            return

        print(f"\nå¯¹è¯å†å² ({len(self.conversation_history)} æ¡æ¶ˆæ¯):")
        for i, msg in enumerate(self.conversation_history, 1):
            role_name = {
                "user": "ç”¨æˆ·",
                "assistant": "åŠ©æ‰‹",
                "tool": "å·¥å…·",
                "system": "ç³»ç»Ÿ"
            }.get(msg.role.value, msg.role.value)

            content_preview = msg.content[:50] + "..." if len(msg.content) > 50 else msg.content
            print(f"  {i}. {role_name}: {content_preview}")

    def _process_user_input(self, user_input: str):
        """Process user input and generate response."""
        try:
            if self.awaiting_user_input:
                # This is a clarification response
                self._handle_clarification(user_input)
            else:
                # This is a new conversation turn
                self._handle_new_turn(user_input)

        except Exception as e:
            logger.error(f"Error processing user input: {e}")
            print(f"å¤„ç†è¾“å…¥æ—¶å‡ºé”™: {e}")

    def _handle_new_turn(self, user_input: str):
        """Handle a new conversation turn using LangGraph."""
        if self.stream:
            print("æ­£åœ¨æ€è€ƒ...", end="", flush=True)
        else:
            print("æ­£åœ¨æ€è€ƒ...", end="", flush=True)

        try:
            if not GRAPH_AVAILABLE:
                print(f"\rLangGraphåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚")
                logger.error("LangGraph not available - import failed during initialization")
                return

            # Create initial state
            initial_state = create_initial_state(user_input)

            # Use full routing logic to determine execution mode
            from agent_core import AgentRouter
            from message_types import ConversationContext

            router = AgentRouter()
            context = ConversationContext(
                conversation_history=self.conversation_history,
                current_tools=[],
                user_preferences={}
            )
            route_decision = router.route(user_input, context)

            # For complex tasks (reasoner), always use planner graph
            if route_decision.engine == "reasoner":
                if self.stream:
                    print(" (æ£€æµ‹åˆ°å¤æ‚ä»»åŠ¡ï¼Œä½¿ç”¨è§„åˆ’æ¨¡å¼)")
                self._handle_planner_execution(user_input, route_decision)
            else:
                # For simple tasks (chat), use streaming if enabled, otherwise regular graph
                if self.stream:
                    self._handle_direct_streaming(user_input)
                else:
                    self._handle_graph_execution(initial_state)

        except Exception as e:
            print(f"\rå¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {e}")
            logger.error(f"Error in conversation turn: {e}")
            # Uncomment for debugging
            # import traceback
            # traceback.print_exc()

    def _handle_graph_execution(self, initial_state):
        """Handle regular graph execution."""
        # Execute the graph with checkpointer configuration
        config = {"configurable": {"thread_id": "cli-session"}}
        final_state = graph_app.invoke(initial_state, config=config)

        # Extract the final answer
        if final_state["final_answer"]:
            print(f"\r{final_state['final_answer']}")
        elif final_state["messages"] and len(final_state["messages"]) > 1:
            # Get the last assistant message
            last_msg = final_state["messages"][-1]
            if last_msg.role == Role.ASSISTANT:
                print(f"\r{last_msg.content}")

        # Update conversation history
        self.conversation_history = final_state["messages"]

        # Log the conversation
        self.logger.log_turn(
            route_decision=final_state.get("route_decision"),
            messages=final_state["messages"],
            tool_calls_count=final_state["tool_call_count"],
            ask_cycles_used=final_state.get("ask_cycles_used", 0)
        )

    def _handle_graph_streaming(self, initial_state):
        """Handle streaming graph execution using LangGraph's stream API."""
        try:
            config = {"configurable": {"thread_id": "cli-session"}}

            # Use LangGraph's stream method for true streaming
            accumulated_content = ""

            for event in graph_app.stream(initial_state, config=config):
                for node_name, node_state in event.items():
                    logger.debug(f"Streaming event from node: {node_name}")

                    # Check if we have a final answer
                    if node_state.get("final_answer"):
                        final_answer = node_state["final_answer"]
                        print(final_answer, end="", flush=True)
                        accumulated_content += final_answer

                    # Check for assistant messages with content
                    elif node_state.get("messages"):
                        messages = node_state["messages"]
                        if messages:
                            last_msg = messages[-1]
                            if (last_msg.role == Role.ASSISTANT and
                                hasattr(last_msg, 'content') and last_msg.content):

                                # Print new content progressively
                                new_content = last_msg.content
                                if len(new_content) > len(accumulated_content):
                                    # Print only the new part
                                    to_print = new_content[len(accumulated_content):]
                                    print(to_print, end="", flush=True)
                                    accumulated_content = new_content

            # Ensure we end with a newline
            if accumulated_content:
                print()

            # Get final state for logging
            final_state = graph_app.get_state(config=config)
            final_state_data = final_state.values

            # Update conversation history
            if final_state_data.get("messages"):
                self.conversation_history = final_state_data["messages"]

            # Log the conversation
            self.logger.log_turn(
                route_decision=final_state_data.get("route_decision"),
                messages=final_state_data.get("messages", []),
                tool_calls_count=final_state_data.get("tool_call_count", 0),
                ask_cycles_used=final_state_data.get("ask_cycles_used", 0)
            )

        except Exception as e:
            print(f"\næµå¼è¾“å‡ºå¤„ç†å‡ºé”™: {e}")
            logger.error(f"Error in streaming graph execution: {e}")
            # Fallback to non-streaming execution
            try:
                self._handle_graph_execution(initial_state)
            except Exception as fallback_error:
                logger.error(f"Fallback execution also failed: {fallback_error}")

    def _handle_direct_streaming(self, user_input: str):
        """Handle streaming using direct LLM calls with tool call support."""
        try:
            from agent_core import AgentCore
            from message_types import Message, Role

            agent = AgentCore()
            streaming_response = agent.process_turn(
                user_input=user_input,
                conversation_history=self.conversation_history,
                stream=True
            )

            # Process streaming chunks with tool call handling
            accumulated_content = ""
            tool_calls_count = 0
            pending_tool_calls = []

            for chunk in streaming_response:
                if chunk.content:
                    print(chunk.content, end="", flush=True)
                    accumulated_content += chunk.content

                if chunk.tool_calls:
                    # Handle tool calls in streaming mode
                    for tool_call in chunk.tool_calls:
                        pending_tool_calls.append(tool_call)
                        tool_calls_count += 1

                        # Execute tool immediately
                        print(f"[æ‰§è¡Œå·¥å…·: {tool_call.name}]", end="", flush=True)
                        try:
                            from tool_registry import registry
                            tool_result = registry.execute(tool_call.name, **tool_call.arguments)

                            # Create tool result message
                            tool_message = Message(
                                role=Role.TOOL,
                                content=f"å·¥å…·æ‰§è¡Œç»“æœ: {tool_result.value if tool_result.ok else tool_result.error}",
                                tool_result=tool_call
                            )

                            # Add to conversation history
                            self.conversation_history.append(tool_message)

                            # Show tool execution in streaming output
                            print(f"\n[æ‰§è¡Œå·¥å…·: {tool_call.name}]", end="", flush=True)

                            # Continue the conversation with tool result
                            # This is a simplified approach - in production, you'd want to
                            # continue the streaming conversation with the tool result
                            follow_up_response = agent.process_turn(
                                user_input="",  # Empty input since we're continuing
                                conversation_history=self.conversation_history,
                                stream=True
                            )

                            # Process follow-up chunks
                            for follow_chunk in follow_up_response:
                                if follow_chunk.content:
                                    print(follow_chunk.content, end="", flush=True)
                                    accumulated_content += follow_chunk.content

                        except Exception as tool_error:
                            logger.error(f"Tool execution failed: {tool_error}")
                            print(f"\n[å·¥å…·æ‰§è¡Œå¤±è´¥: {tool_call.name}]", end="", flush=True)

            # Ensure we end with a newline
            if accumulated_content:
                print()

            # Add user and assistant messages to history
            self.conversation_history.append(Message(role=Role.USER, content=user_input))
            if accumulated_content:
                self.conversation_history.append(Message(
                    role=Role.ASSISTANT,
                    content=accumulated_content
                ))

            # Log the conversation
            from agent_core import RouteDecision
            dummy_route_decision = RouteDecision(
                engine="chat",
                reason="Streaming mode with tool calls",
                confidence=1.0
            )
            self.logger.log_turn(
                route_decision=dummy_route_decision,
                messages=self.conversation_history[-2:],
                tool_calls_count=tool_calls_count,
                ask_cycles_used=0
            )

        except Exception as e:
            print(f"\nç›´æ¥æµå¼è¾“å‡ºå¤„ç†å‡ºé”™: {e}")
            logger.error(f"Error in direct streaming: {e}")
            # Fallback to non-streaming
            initial_state = create_initial_state(user_input)
            self._handle_graph_execution(initial_state)

    def _handle_planner_execution(self, user_input: str, route_decision):
        """Handle planner execution using LangGraph planner with heartbeat monitoring."""
        try:
            # Create initial state for planner
            initial_state = create_initial_state(user_input)

            # Execute planner graph with heartbeat monitoring
            config = {
                "configurable": {"thread_id": f"planner-{user_input[:20]}"},  # Use input prefix for thread ID
                "recursion_limit": 100  # Increase from default 25 to handle complex planning
            }

            # Hotfix-4: Add heartbeat logging for planner execution monitoring
            import time
            start_time = time.time()
            logger.info("Starting planner execution...")

            # Use stream mode to monitor progress and prevent blocking
            final_state = None
            try:
                # Try streaming mode first for better monitoring
                accumulated_state = initial_state.copy()
                for event in planner_app.stream(initial_state, config=config):
                    for node_name, node_state in event.items():
                        accumulated_state.update(node_state)

                        # Check for planner generation completion
                        if node_name == "planner_generate" and node_state.get("plan"):
                            logger.info(f"Planner generated {len(node_state['plan'])} todos")

                        # ğŸ”´ å…³é”®ï¼šæ•æ‰ ask_user_interrupt ä¸­æ–­å¹¶é˜»å¡ç­‰å¾…è¾“å…¥
                        if node_name == "ask_user_interrupt" and node_state.get("needs_user_input"):
                            logger.info("ğŸ›‘ DETECTED USER INPUT REQUIREMENT - Blocking for user input")
                            needs_info = node_state["needs_user_input"]
                            prompt = f"éœ€è¦ä¸º '{needs_info.get('todo_title', 'ä»»åŠ¡')}' æä¾›å‚æ•°: {', '.join(needs_info.get('needs', []))}"

                            print(f"\n\033[33m[USER INPUT REQUIRED]\033[0m {prompt}")
                            try:
                                # é˜»å¡ç­‰å¾…ç”¨æˆ·è¾“å…¥ï¼Œæœ€å¤šç­‰å¾…120ç§’
                                import select
                                import sys

                                print("> ", end="", flush=True)
                                ready, _, _ = select.select([sys.stdin], [], [], 120.0)

                                if ready:
                                    user_text = input().strip()
                                else:
                                    print("\nè¾“å…¥è¶…æ—¶ï¼Œä½¿ç”¨é»˜è®¤å€¼ç»§ç»­...")
                                    user_text = ""  # è¶…æ—¶é™çº§

                            except (EOFError, KeyboardInterrupt):
                                print("\næ“ä½œå–æ¶ˆ")
                                user_text = ""

                            # å°†è¾“å…¥å†™å›çŠ¶æ€å¹¶åŒæ­¥æ¨è¿›
                            accumulated_state["user_provided_input"] = {param: user_text for param in needs_info.get('needs', [])}
                            accumulated_state["needs_info"] = needs_info

                            # åŒæ­¥è°ƒç”¨ä»¥å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶ç»§ç»­æ‰§è¡Œ
                            accumulated_state = planner_app.invoke(accumulated_state, config=config)
                            logger.info("âœ… User input injected and graph resumed")
                            continue

                        # Check for completion
                        if node_state.get("should_end") or node_state.get("final_answer"):
                            final_state = accumulated_state.copy()
                            # Ensure needs_user_input is included in final state
                            if node_state.get("needs_user_input"):
                                final_state["needs_user_input"] = node_state["needs_user_input"]
                                # Save state for continuation if user input is needed
                                self.last_planner_state = final_state.copy()
                            break

                    # Safety timeout check (5 minutes max)
                    if time.time() - start_time > 300:
                        logger.error("Planner execution timeout after 5 minutes")
                        raise TimeoutError("Planner execution exceeded 5 minute timeout")

                if final_state is None:
                    # Fallback to invoke if streaming didn't complete
                    logger.warning("Planner streaming didn't complete, falling back to invoke")
                    final_state = planner_app.invoke(initial_state, config=config)

            except Exception as stream_error:
                logger.warning(f"Planner streaming failed: {stream_error}, falling back to invoke")
                final_state = planner_app.invoke(initial_state, config=config)

            execution_time = time.time() - start_time
            logger.info(f"Planner execution completed in {execution_time:.2f}s")

            # Check if user input is needed before showing results
            if final_state.get("needs_user_input"):
                logger.info("User input required, triggering parameter collection")
                self._handle_parameter_collection(final_state["needs_user_input"])
                return

            # Extract the final answer
            if final_state["final_answer"]:
                print(final_state["final_answer"])
            elif final_state["messages"]:
                # Get the last assistant message
                last_msg = final_state["messages"][-1]
                if hasattr(last_msg, 'content') and last_msg.content:
                    print(last_msg.content)

            # Update conversation history
            if final_state.get("messages"):
                self.conversation_history = final_state["messages"]

            # Log the conversation
            self.logger.log_turn(
                route_decision=route_decision,
                messages=final_state.get("messages", []),
                tool_calls_count=final_state.get("tool_call_count", 0),
                ask_cycles_used=0  # Planner handles ask cycles internally
            )

        except Exception as e:
            print(f"\nè§„åˆ’æ‰§è¡Œå‡ºé”™: {e}")
            logger.error(f"Error in planner execution: {e}")
            # Fallback to direct streaming
            self._handle_direct_streaming(user_input)

    def _handle_regular_response(self, result):
        """Handle regular (non-streaming) response."""
        # Update conversation history
        self.conversation_history = result["final_messages"]

        # Log the conversation turn
        self.logger.log_turn(
            route_decision=result["route_decision"],
            messages=result["final_messages"],
            tool_calls_count=result["tool_calls_made"],
            ask_cycles_used=result["ask_cycles_used"]
        )

        # Handle response
        if result.get("awaiting_user_input"):
            self.awaiting_user_input = True
            print(f"\r{result['response']}")
        elif result.get("needs_user_input"):
            # New mechanism: collect specific parameters from user
            self._handle_parameter_collection(result["needs_user_input"])
        else:
            self.awaiting_user_input = False
            self._display_response(result)

    def _handle_streaming_response(self, result_generator, user_input: str):
        """Handle streaming response."""
        accumulated_content = ""
        tool_calls_made = 0
        ask_cycles_used = 0

        try:
            for chunk in result_generator:
                # Print content chunks
                if chunk.content:
                    print(chunk.content, end="", flush=True)
                    accumulated_content += chunk.content

                # Handle tool calls
                if chunk.tool_calls:
                    tool_calls_made += len(chunk.tool_calls)

                # Handle finish
                if chunk.finish_reason:
                    if chunk.finish_reason == "stop":
                        print()  # New line at the end
                    elif chunk.finish_reason == "error":
                        print(f"\næµå¼è¾“å‡ºå‡ºé”™: {chunk.content}")
                        return

            # Create a synthetic result for logging
            route_decision = RouteDecision(
                engine="streaming",
                reason="streaming_response",
                confidence=1.0
            )

            # Log the conversation turn
            self.logger.log_turn(
                route_decision=route_decision,
                messages=[
                    Message(role=Role.USER, content=user_input),
                    Message(role=Role.ASSISTANT, content=accumulated_content)
                ],
                tool_calls_count=tool_calls_made,
                ask_cycles_used=ask_cycles_used
            )

            # Update conversation history
            self.conversation_history = [
                Message(role=Role.USER, content=user_input),
                Message(role=Role.ASSISTANT, content=accumulated_content)
            ]

        except Exception as e:
            print(f"\næµå¼è¾“å‡ºå¤„ç†å‡ºé”™: {e}")
            logger.error(f"Error handling streaming response: {e}")

    def _show_graph_status(self):
        """Show current graph execution status."""
        print("\n=== LangGraph æ‰§è¡ŒçŠ¶æ€ ===")
        print(f"å½“å‰èŠ‚ç‚¹: user_input (ç­‰å¾…ç”¨æˆ·è¾“å…¥)")
        print(f"å¯¹è¯è½®æ•°: {len([msg for msg in self.conversation_history if msg.role == Role.USER])}")
        print(f"æ¶ˆæ¯æ€»æ•°: {len(self.conversation_history)}")
        print(f"ç­‰å¾…ç”¨æˆ·è¾“å…¥: {getattr(self, 'awaiting_user_input', False)}")
        print(f"æµå¼è¾“å‡º: {self.stream}")
        print("æ‰§è¡Œè·¯å¾„: user_input â†’ decide_route â†’ model_call â†’ [tool_exec|need_user|end]")
        print("=" * 30)

    def _show_route_status(self):
        """Show current routing decision."""
        print("\n=== è·¯ç”±å†³ç­–çŠ¶æ€ ===")
        print("æ”¯æŒçš„è·¯ç”±æ¨¡å¼:")
        print("  â€¢ chat: ç®€å•å¯¹è¯å’ŒåŸºç¡€å·¥å…·è°ƒç”¨")
        print("  â€¢ reasoner: å¤æ‚æ¨ç†å’Œæ•°å­¦è®¡ç®—")
        print("  â€¢ planner: å¤æ‚è§„åˆ’å’Œå¤šæ­¥éª¤ä»»åŠ¡")
        print("  â€¢ auto_rag: è‡ªåŠ¨çŸ¥è¯†æ‰©å……")
        print("\nå†³ç­–ä¾æ®:")
        print("  â€¢ å…³é”®è¯æ£€æµ‹ï¼ˆè®¡åˆ’ã€æ–¹æ¡ˆã€å¤šæ­¥éª¤ç­‰ï¼‰")
        print("  â€¢ æ–‡æœ¬é•¿åº¦é˜ˆå€¼ï¼ˆ>100å­—ç¬¦å€¾å‘è§„åˆ’ï¼‰")
        print("  â€¢ ç»“æ„åŒ–æ¨¡å¼ï¼ˆæ•°å­—åˆ—è¡¨ã€æµç¨‹å›¾ç­‰ï¼‰")
        print("  â€¢ å¤æ‚åº¦è¯„ä¼°")
        print("=" * 30)

    def _show_prompt_status(self, command: str):
        """Show current system prompt and tools."""
        parts = command.split()
        show_system = len(parts) <= 1 or 'system' in parts[1:]
        show_tools = len(parts) <= 1 or 'tools' in parts[1:]

        print("\n=== ç³»ç»Ÿæç¤ºçŠ¶æ€ ===")

        if show_system:
            print("\nğŸ“ ç³»ç»Ÿæç¤º:")
            try:
                from tool_prompt_builder import load_system_prompt
                system_prompt = load_system_prompt("chat")  # Default to chat
                # Show first 200 chars and indicate if truncated
                if len(system_prompt) > 200:
                    print(f"{system_prompt[:200]}...")
                    print(f"\n(å®Œæ•´æç¤ºé•¿åº¦: {len(system_prompt)} å­—ç¬¦)")
                else:
                    print(system_prompt)
            except Exception as e:
                print(f"åŠ è½½ç³»ç»Ÿæç¤ºå¤±è´¥: {e}")

        if show_tools:
            print("\nğŸ› ï¸ å·¥å…·æ¸…å•:")
            try:
                from tool_prompt_builder import build_tool_prompts
                tool_prompts = build_tool_prompts()
                print(f"å·²æ³¨å†Œå·¥å…·æ•°é‡: {len(tool_prompts['tools'])}")
                for tool in tool_prompts["tool_name_index"]:
                    desc = tool_prompts["tool_name_index"][tool]["description"]
                    print(f"  â€¢ {tool}: {desc[:50]}{'...' if len(desc) > 50 else ''}")
            except Exception as e:
                print(f"åŠ è½½å·¥å…·æ¸…å•å¤±è´¥: {e}")

        print("=" * 30)

    def _show_plan_status(self):
        """Show current plan details."""
        print("\n=== å½“å‰è®¡åˆ’è¯¦æƒ… ===")

        # This would ideally access the current graph state
        # For now, show general information
        print("Planner v1.0: æ”¯æŒç»“æ„åŒ–ä»»åŠ¡è§„åˆ’ä¸æ‰§è¡Œ")
        print("è®¡åˆ’ç”Ÿæˆ: Reasoneræ¨¡å‹ + JSONæ¨¡å¼")
        print("æ‰§è¡Œåˆ†å‘: æ™ºèƒ½è·¯ç”±åˆ°Chat/Reasoneræ‰§è¡Œè€…")
        print("å·¥å…·è°ƒç”¨: ä¸¥æ ¼ä¸¤æ­¥åè®® (tool_calls â†’ role:tool â†’ ç»§ç»­)")
        print("\nè®¡åˆ’åŒ…å«å­—æ®µ:")
        print("  â€¢ id: å”¯ä¸€æ ‡è¯†ç¬¦")
        print("  â€¢ title: ä»»åŠ¡æ ‡é¢˜")
        print("  â€¢ type: tool/chat/reason/write/research")
        print("  â€¢ executor: auto/chat/reasoner")
        print("  â€¢ tool: å·¥å…·åç§° (type=toolæ—¶)")
        print("  â€¢ input: è¾“å…¥å‚æ•°")
        print("  â€¢ expected_output: æœŸæœ›è¾“å‡º")
        print("  â€¢ needs: ç¼ºå¤±ä¿¡æ¯éœ€æ±‚")
        print("\næ‰§è¡Œè€…é€‰æ‹©ä¼˜å…ˆçº§:")
        print("  1. TodoItem.executor (æ˜¾å¼æŒ‡å®š)")
        print("  2. TOOL_META.executor_default")
        print("  3. å¤æ‚åº¦è‡ªåŠ¨åˆ¤æ–­")
        print("  4. Chat (é»˜è®¤)")
        print("=" * 30)

    def _show_todo_details(self, todo_id: str):
        """Show details for a specific todo item."""
        print(f"\n=== TODO {todo_id} è¯¦æƒ… ===")

        # This would access the current plan from graph state
        # For now, show placeholder information
        print(f"TODO ID: {todo_id}")
        print("çŠ¶æ€: è®¡åˆ’ä¸­ (å®é™…è¿è¡Œæ—¶ä¼šæ˜¾ç¤ºæ‰§è¡ŒçŠ¶æ€)")
        print("\nå­—æ®µè¯´æ˜:")
        print("  â€¢ title: ä»»åŠ¡å…·ä½“æè¿°")
        print("  â€¢ type: æ‰§è¡Œç±»å‹ (tool/chat/reason/write/research)")
        print("  â€¢ executor: æ‰§è¡Œè€… (chat/reasoner)")
        print("  â€¢ tool: å·¥å…·åç§° (type=toolæ—¶)")
        print("  â€¢ input: è¾“å…¥å‚æ•°")
        print("  â€¢ expected_output: æœŸæœ›äº§å‡ºæ ¼å¼")
        print("  â€¢ output: å®é™…æ‰§è¡Œç»“æœ")
        print("  â€¢ needs: ç¼ºå¤±ä¿¡æ¯éœ€æ±‚")
        print("\nåœ¨å®é™…è¿è¡Œä¸­ï¼Œæ­¤å‘½ä»¤ä¼šæ˜¾ç¤ºè¯¥TODOçš„å®Œæ•´çŠ¶æ€ä¿¡æ¯ã€‚")
        print("=" * 30)

    def _handle_clarification(self, clarification: str):
        """Handle user clarification after AskUser."""
        print("æ­£åœ¨ç»§ç»­å¤„ç†...", end="", flush=True)

        try:
            # Get the last route decision from history
            # In a more sophisticated implementation, we'd store this in context
            # For now, we'll re-route based on the clarification
            from .agent_core import RouteDecision

            # Simple heuristic: check if clarification is complex
            route_decision = agent.router.route(clarification, None)

            # Continue with clarification
            result = agent.continue_with_user_clarification(
                clarification,
                self.conversation_history,
                route_decision
            )

            # Update conversation history
            self.conversation_history = result["final_messages"]

            # Log continuation
            self.logger.log_continuation(
                clarification=clarification,
                messages=result["final_messages"],
                tool_calls_count=result["tool_calls_made"]
            )

            # Reset state and display response
            self.awaiting_user_input = False
            self._display_response(result)

        except Exception as e:
            print(f"\rå¤„ç†æ¾„æ¸…ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            logger.error(f"Error handling clarification: {e}")
            self.awaiting_user_input = False

    def _handle_parameter_collection(self, needs_info: dict):
        """Handle collection of specific parameters from user."""
        print(f"\nğŸ” éœ€è¦æ”¶é›†å‚æ•°ç”¨äº: {needs_info['todo_title']}")
        print(f"è¯·æä¾›ä»¥ä¸‹å‚æ•°:")

        collected_params = {}
        for param in needs_info["needs"]:
            while True:
                try:
                    value = input(f"  {param}: ").strip()
                    if value:
                        collected_params[param] = value
                        break
                    else:
                        print(f"  âŒ {param}ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
                except (EOFError, KeyboardInterrupt):
                    print("\næ“ä½œå–æ¶ˆ")
                    return

        print("âœ… å‚æ•°æ”¶é›†å®Œæˆï¼Œæ­£åœ¨ç»§ç»­æ‰§è¡Œ...")  # Continue with collected parameters
        self._continue_with_parameters(collected_params, needs_info)

    def _continue_with_parameters(self, parameters: dict, needs_info: dict):
        """Continue execution with collected parameters."""
        try:
            # Check if we have a saved planner state to continue
            if self.last_planner_state and self.last_planner_state.get("plan"):
                # Continue the previous planner execution with updated parameters
                self._continue_planner_execution(parameters, needs_info)
                return

            # Fallback to agent method for non-planner continuations
            result = agent.continue_with_parameter_input(
                parameters,
                self.conversation_history,
                needs_info
            )

            # Since this is a continuation of planner execution,
            # we need to handle the response differently
            if result.get("awaiting_user_input") or result.get("needs_user_input"):
                # Still needs more input
                if result.get("awaiting_user_input"):
                    self.awaiting_user_input = True
                    print(f"\r{result['response']}")
                elif result.get("needs_user_input"):
                    self._handle_parameter_collection(result["needs_user_input"])
                return

            # Execution completed
            self._display_response(result)

            # Update conversation history
            self.conversation_history = result["final_messages"]

            # Log continuation
            self.logger.log_continuation(
                clarification=f"Parameter input: {parameters}",
                messages=result["final_messages"],
                tool_calls_count=result["tool_calls_made"]
            )

            # Display response
            self._display_response(result)

        except Exception as e:
            print(f"å¤„ç†å‚æ•°è¾“å…¥æ—¶å‡ºé”™: {e}")
            logger.error(f"Error handling parameter input: {e}")

    def _continue_planner_execution(self, parameters: dict, needs_info: dict):
        """Continue a previously interrupted planner execution with user parameters."""
        try:
            if not self.last_planner_state:
                raise ValueError("No saved planner state to continue")

            # Update the saved state with user provided parameters
            updated_state = self.last_planner_state.copy()

            # Set up state for ask_user_interrupt_node to process
            updated_state["user_provided_input"] = parameters
            updated_state["needs_info"] = needs_info

            # Clear the needs_user_input flag since we have the parameters
            updated_state["needs_user_input"] = False

            # Add a system message indicating parameter collection completion
            param_summary = ", ".join([f"{k}='{v}'" for k, v in parameters.items()])
            system_message = Message(
                role=Role.SYSTEM,
                content=f"ç”¨æˆ·å·²æä¾›æ‰€éœ€å‚æ•°: {param_summary}ã€‚ç»§ç»­æ‰§è¡Œè®¡åˆ’ã€‚"
            )
            updated_state["messages"].append(system_message)

            logger.info(f"Continuing planner execution with parameters: {parameters}")

            # Continue the planner execution from the updated state
            config = {
                "configurable": {"thread_id": f"planner-continuation-{hash(str(parameters))}"},
                "recursion_limit": 100
            }

            # Execute the planner from the updated state
            final_state = planner_app.invoke(updated_state, config=config)

            # Handle the final result
            if final_state.get("final_answer"):
                print(final_state["final_answer"])
            elif final_state["messages"]:
                # Get the last assistant message
                for msg in reversed(final_state["messages"]):
                    if msg.role == Role.ASSISTANT:
                        print(msg.content)
                        break

            # Update conversation history
            self.conversation_history = final_state["messages"]

            # Clear the saved state since execution is complete
            self.last_planner_state = None

            # Log the completion
            self.logger.log_continuation(
                clarification=f"Planner continuation with parameters: {parameters}",
                messages=final_state["messages"],
                tool_calls_count=final_state.get("tool_call_count", 0)
            )

        except Exception as e:
            print(f"ç»§ç»­è§„åˆ’æ‰§è¡Œæ—¶å‡ºé”™: {e}")
            logger.error(f"Error continuing planner execution: {e}")
            # Reset state on error
            self.last_planner_state = None

    def _display_response(self, result: dict):
        """Display the final response."""
        response = result["response"]

        if self.stream:
            # Simulate streaming by printing character by character
            print("\r", end="")
            for char in response:
                print(char, end="", flush=True)
                # Small delay for streaming effect (optional)
        else:
            print(f"\r{response}")

        # Show statistics if available
        tool_calls = result.get("tool_calls_made", 0)
        if tool_calls > 0:
            print(f"\n[ä½¿ç”¨äº† {tool_calls} ä¸ªå·¥å…·è°ƒç”¨]")

    def cleanup(self):
        """Cleanup resources."""
        if hasattr(self.logger, 'close'):
            self.logger.close()


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="èµ›åšå…‹éš†AIåŠ©æ‰‹ CLI")
    parser.add_argument(
        "--stream",
        action="store_true",
        help="å¯ç”¨æµå¼è¾“å‡º"
    )

    args = parser.parse_args()

    app = CLIApp(stream=args.stream)
    try:
        app.run()
    finally:
        app.cleanup()


if __name__ == "__main__":
    main()
