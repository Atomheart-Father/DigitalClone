#!/usr/bin/env python3
"""
CLI Application for the Digital Clone AI Assistant.

Provides a command-line REPL interface with support for streaming output,
special commands, and AskUser state management.
"""

import sys
import argparse
import logging
from typing import List, Optional

import config
import message_types
import logger

# Import graph modules with proper error handling
try:
    from graph import graph_app, planner_app
    from graph.state import create_initial_state
    from tool_prompt_builder import load_system_prompt
    GRAPH_AVAILABLE = True
except ImportError as e:
    logger.logger.warning(f"Graph modules not available: {e}")
    GRAPH_AVAILABLE = False
    # Fallback imports for compatibility
    from tool_prompt_builder import load_system_prompt

# ä¸ºäº†å…¼å®¹æ€§ï¼Œåˆ›å»ºåˆ«å
config = config.config
Message = message_types.Message
Role = message_types.Role
ConversationLogger = logger.ConversationLogger

# Configure logging
logging.basicConfig(
    level=getattr(logging, config.LOG_LEVEL),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CLIApp:
    """Command-line interface for the AI assistant."""

    def __init__(self, stream: bool = False):
        self.stream = stream
        self.conversation_history: List[Message] = []
        self.awaiting_user_input = False
        self.logger = ConversationLogger()

    def run(self):
        """Main CLI application loop."""
        self._print_welcome()

        try:
            while True:
                if self.awaiting_user_input:
                    prompt = "è¯·æä¾›æ›´å¤šä¿¡æ¯> "
                else:
                    prompt = "> "

                try:
                    user_input = input(prompt).strip()
                except (EOFError, KeyboardInterrupt):
                    print("\nå†è§ï¼")
                    break

                if not user_input:
                    continue

                # Handle special commands
                if user_input.startswith(':'):
                    if self._handle_special_command(user_input):
                        break
                    continue

                # Process user input
                self._process_user_input(user_input)

        except Exception as e:
            logger.error(f"CLI application error: {e}")
            print(f"\nå‘ç”Ÿé”™è¯¯: {e}")
            print("è¯·æ£€æŸ¥æ—¥å¿—ä»¥è·å–è¯¦ç»†ä¿¡æ¯ã€‚")

    def _print_welcome(self):
        """Print welcome message and system information."""
        print("=" * 50)
        print("ğŸ­ èµ›åšå…‹éš† AI åŠ©æ‰‹")
        print("=" * 50)

        # Show model information
        if config.is_api_key_available():
            print(f"âœ“ å·²é…ç½® API å¯†é’¥")
            print(f"ğŸ“¡ Chatæ¨¡å‹: {config.MODEL_CHAT}")
            print(f"ğŸ¤– Reasoneræ¨¡å‹: {config.MODEL_REASONER}")
        else:
            print("âš ï¸  æœªé…ç½® API å¯†é’¥ï¼Œä½¿ç”¨ MockClient")

        # Show tool information
        from tool_registry import registry
        tools = registry.list_tools()
        print(f"ğŸ› ï¸  å·²åŠ è½½ {len(tools)} ä¸ªå·¥å…·: {[t.name for t in tools]}")

        print("\nè¾“å…¥æ‚¨çš„æ¶ˆæ¯ï¼Œæˆ–ä½¿ç”¨ç‰¹æ®Šå‘½ä»¤:")
        print("  :help  æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
        print("  :tools åˆ—å‡ºå¯ç”¨å·¥å…·")
        print("  :q     é€€å‡ºç¨‹åº")
        print("  :clear æ¸…ç©ºå¯¹è¯å†å²")
        print("  :graph æ˜¾ç¤ºå›¾çŠ¶æ€")
        print("  :route æ˜¾ç¤ºè·¯ç”±å†³ç­–")
        print("  :prompt [system|tools] æ˜¾ç¤ºç³»ç»Ÿæç¤º")
        print("  :plan  æ˜¾ç¤ºè®¡åˆ’è¯¦æƒ…")
        print("  :todo <id> æ˜¾ç¤ºç‰¹å®šTODOè¯¦æƒ…")
        print("-" * 50)

    def _handle_special_command(self, command: str) -> bool:
        """
        Handle special commands starting with ':'.

        Returns:
            True if should exit, False otherwise
        """
        cmd = command[1:].lower()

        if cmd == 'q' or cmd == 'quit':
            print("å†è§ï¼")
            return True

        elif cmd == 'help':
            self._show_help()
            return False

        elif cmd == 'tools':
            self._show_tools()
            return False

        elif cmd == 'clear':
            self.conversation_history.clear()
            self.awaiting_user_input = False
            print("âœ“ å·²æ¸…ç©ºå¯¹è¯å†å²")
            return False

        elif cmd == 'history':
            self._show_history()
            return False

        elif cmd == 'graph':
            self._show_graph_status()
            return False

        elif cmd == 'route':
            self._show_route_status()
            return False

        elif cmd == 'prompt':
            self._show_prompt_status(command)
            return False

        elif cmd == 'plan':
            self._show_plan_status()
            return False

        elif cmd.startswith('todo'):
            # :todo <id> - show specific todo details
            parts = cmd.split()
            if len(parts) == 2:
                todo_id = parts[1]
                self._show_todo_details(todo_id)
            else:
                print("ç”¨æ³•: :todo <id>")
                print("ç¤ºä¾‹: :todo T1")
            return False

        else:
            print(f"æœªçŸ¥å‘½ä»¤: {command}")
            print("è¾“å…¥ :help æŸ¥çœ‹å¯ç”¨å‘½ä»¤")
            return False

    def _show_help(self):
        """Show help information."""
        print("\nå¸®åŠ©ä¿¡æ¯:")
        print("  å¸¸è§„å¯¹è¯: ç›´æ¥è¾“å…¥æ‚¨çš„é—®é¢˜")
        print("  ç‰¹æ®Šå‘½ä»¤:")
        print("    :help   - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
        print("    :tools  - åˆ—å‡ºå¯ç”¨å·¥å…·")
        print("    :q      - é€€å‡ºç¨‹åº")
        print("    :clear  - æ¸…ç©ºå¯¹è¯å†å²")
        print("    :history- æ˜¾ç¤ºå¯¹è¯å†å²")
        print("\n  æµå¼è¾“å‡º: ä½¿ç”¨ --stream å‚æ•°å¯ç”¨")
        print("  AskUser: å½“åŠ©æ‰‹éœ€è¦æ›´å¤šä¿¡æ¯æ—¶ï¼Œä¼šæç¤ºæ‚¨æä¾›")

    def _show_tools(self):
        """Show available tools."""
        from tool_registry import registry
        tools = registry.list_tools()
        if not tools:
            print("æš‚æ— å¯ç”¨å·¥å…·")
            return

        print(f"\nå¯ç”¨å·¥å…· ({len(tools)} ä¸ª):")
        for tool in tools:
            print(f"  â€¢ {tool.name}: {tool.description}")
            # Show parameters if available
            if hasattr(tool, 'parameters') and tool.parameters.get('properties'):
                params = list(tool.parameters['properties'].keys())
                print(f"    å‚æ•°: {', '.join(params)}")
            # Show executor info
            if hasattr(tool, 'executor_default'):
                print(f"    æ‰§è¡Œè€…: {tool.executor_default}")
            if hasattr(tool, 'complexity'):
                print(f"    å¤æ‚åº¦: {tool.complexity}")

    def _show_history(self):
        """Show conversation history."""
        if not self.conversation_history:
            print("æš‚æ— å¯¹è¯å†å²")
            return

        print(f"\nå¯¹è¯å†å² ({len(self.conversation_history)} æ¡æ¶ˆæ¯):")
        for i, msg in enumerate(self.conversation_history, 1):
            role_name = {
                "user": "ç”¨æˆ·",
                "assistant": "åŠ©æ‰‹",
                "tool": "å·¥å…·",
                "system": "ç³»ç»Ÿ"
            }.get(msg.role.value, msg.role.value)

            content_preview = msg.content[:50] + "..." if len(msg.content) > 50 else msg.content
            print(f"  {i}. {role_name}: {content_preview}")

    def _process_user_input(self, user_input: str):
        """Process user input and generate response."""
        try:
            if self.awaiting_user_input:
                # This is a clarification response
                self._handle_clarification(user_input)
            else:
                # This is a new conversation turn
                self._handle_new_turn(user_input)

        except Exception as e:
            logger.error(f"Error processing user input: {e}")
            print(f"å¤„ç†è¾“å…¥æ—¶å‡ºé”™: {e}")

    def _handle_new_turn(self, user_input: str):
        """Handle a new conversation turn using LangGraph."""
        if self.stream:
            print("æ­£åœ¨æ€è€ƒ...", end="", flush=True)
        else:
            print("æ­£åœ¨æ€è€ƒ...", end="", flush=True)

        try:
            if not GRAPH_AVAILABLE:
                print(f"\rLangGraphåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚")
                logger.error("LangGraph not available - import failed during initialization")
                return

            # Create initial state
            initial_state = create_initial_state(user_input)

            # Quick route classification for streaming mode
            if self.stream:
                # Check if it's a complex planning task for streaming mode
                user_lower = user_input.lower()
                is_complex_planning = any(keyword in user_lower for keyword in [
                    'è®¡åˆ’', 'è§„åˆ’', 'åˆ¶å®š', 'å¤šæ­¥éª¤', 'è°ƒç ”', 'æ–¹æ¡ˆ', 'è¯„ä¼°',
                    'å¯¹æ¯”', 'æµç¨‹', 'ä¾èµ–', 'é˜¶æ®µ', 'é¡¹ç›®', 'ä»»åŠ¡åˆ†è§£'
                ]) or len(user_input) > 100  # Lower threshold for streaming

                if is_complex_planning:
                    # Use planner graph for complex planning tasks
                    from agent_core import RouteDecision
                    dummy_route_decision = RouteDecision(
                        engine="reasoner",
                        reason="Complex planning task - using planner graph",
                        confidence=1.0
                    )
                    self._handle_planner_execution(user_input, dummy_route_decision)
                else:
                    # Use direct streaming for simple tasks
                    self._handle_direct_streaming(user_input)
            else:
                # Check if it's a complex planning task for non-streaming mode
                user_lower = user_input.lower()
                is_complex_planning = any(keyword in user_lower for keyword in [
                    'è®¡åˆ’', 'è§„åˆ’', 'åˆ¶å®š', 'å¤šæ­¥éª¤', 'è°ƒç ”', 'æ–¹æ¡ˆ', 'è¯„ä¼°',
                    'å¯¹æ¯”', 'æµç¨‹', 'ä¾èµ–', 'é˜¶æ®µ', 'é¡¹ç›®', 'ä»»åŠ¡åˆ†è§£'
                ]) or len(user_input) > 150  # Higher threshold for non-streaming

                if is_complex_planning:
                    # Use planner graph for complex tasks in non-streaming mode
                    from agent_core import RouteDecision
                    dummy_route_decision = RouteDecision(
                        engine="reasoner",
                        reason="Complex planning task - using planner graph",
                        confidence=1.0
                    )
                    self._handle_planner_execution(user_input, dummy_route_decision)
                else:
                    # Handle regular graph execution
                    self._handle_graph_execution(initial_state)

        except Exception as e:
            print(f"\rå¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {e}")
            logger.error(f"Error in conversation turn: {e}")
            # Uncomment for debugging
            # import traceback
            # traceback.print_exc()

    def _handle_graph_execution(self, initial_state):
        """Handle regular graph execution."""
        # Execute the graph with checkpointer configuration
        config = {"configurable": {"thread_id": "cli-session"}}
        final_state = graph_app.invoke(initial_state, config=config)

        # Extract the final answer
        if final_state["final_answer"]:
            print(f"\r{final_state['final_answer']}")
        elif final_state["messages"] and len(final_state["messages"]) > 1:
            # Get the last assistant message
            last_msg = final_state["messages"][-1]
            if last_msg.role == Role.ASSISTANT:
                print(f"\r{last_msg.content}")

        # Update conversation history
        self.conversation_history = final_state["messages"]

        # Log the conversation
        self.logger.log_turn(
            route_decision=final_state.get("route_decision"),
            messages=final_state["messages"],
            tool_calls_count=final_state["tool_call_count"],
            ask_cycles_used=final_state.get("ask_cycles_used", 0)
        )

    def _handle_graph_streaming(self, initial_state):
        """Handle streaming graph execution using LangGraph's stream API."""
        try:
            config = {"configurable": {"thread_id": "cli-session"}}

            # Use LangGraph's stream method for true streaming
            accumulated_content = ""

            for event in graph_app.stream(initial_state, config=config):
                for node_name, node_state in event.items():
                    logger.debug(f"Streaming event from node: {node_name}")

                    # Check if we have a final answer
                    if node_state.get("final_answer"):
                        final_answer = node_state["final_answer"]
                        print(final_answer, end="", flush=True)
                        accumulated_content += final_answer

                    # Check for assistant messages with content
                    elif node_state.get("messages"):
                        messages = node_state["messages"]
                        if messages:
                            last_msg = messages[-1]
                            if (last_msg.role == Role.ASSISTANT and
                                hasattr(last_msg, 'content') and last_msg.content):

                                # Print new content progressively
                                new_content = last_msg.content
                                if len(new_content) > len(accumulated_content):
                                    # Print only the new part
                                    to_print = new_content[len(accumulated_content):]
                                    print(to_print, end="", flush=True)
                                    accumulated_content = new_content

            # Ensure we end with a newline
            if accumulated_content:
                print()

            # Get final state for logging
            final_state = graph_app.get_state(config=config)
            final_state_data = final_state.values

            # Update conversation history
            if final_state_data.get("messages"):
                self.conversation_history = final_state_data["messages"]

            # Log the conversation
            self.logger.log_turn(
                route_decision=final_state_data.get("route_decision"),
                messages=final_state_data.get("messages", []),
                tool_calls_count=final_state_data.get("tool_call_count", 0),
                ask_cycles_used=final_state_data.get("ask_cycles_used", 0)
            )

        except Exception as e:
            print(f"\næµå¼è¾“å‡ºå¤„ç†å‡ºé”™: {e}")
            logger.error(f"Error in streaming graph execution: {e}")
            # Fallback to non-streaming execution
            try:
                self._handle_graph_execution(initial_state)
            except Exception as fallback_error:
                logger.error(f"Fallback execution also failed: {fallback_error}")

    def _handle_direct_streaming(self, user_input: str):
        """Handle streaming using direct LLM calls with tool call support."""
        try:
            from agent_core import AgentCore
            from message_types import Message, Role

            agent = AgentCore()
            streaming_response = agent.process_turn(
                user_input=user_input,
                conversation_history=self.conversation_history,
                stream=True
            )

            # Process streaming chunks with tool call handling
            accumulated_content = ""
            tool_calls_count = 0
            pending_tool_calls = []

            for chunk in streaming_response:
                if chunk.content:
                    print(chunk.content, end="", flush=True)
                    accumulated_content += chunk.content

                if chunk.tool_calls:
                    # Handle tool calls in streaming mode
                    for tool_call in chunk.tool_calls:
                        pending_tool_calls.append(tool_call)
                        tool_calls_count += 1

                        # Execute tool immediately
                        print(f"[æ‰§è¡Œå·¥å…·: {tool_call.name}]", end="", flush=True)
                        try:
                            from tool_registry import registry
                            tool_result = registry.execute(tool_call.name, **tool_call.arguments)

                            # Create tool result message
                            tool_message = Message(
                                role=Role.TOOL,
                                content=f"å·¥å…·æ‰§è¡Œç»“æœ: {tool_result.value if tool_result.ok else tool_result.error}",
                                tool_result=tool_call
                            )

                            # Add to conversation history
                            self.conversation_history.append(tool_message)

                            # Show tool execution in streaming output
                            print(f"\n[æ‰§è¡Œå·¥å…·: {tool_call.name}]", end="", flush=True)

                            # Continue the conversation with tool result
                            # This is a simplified approach - in production, you'd want to
                            # continue the streaming conversation with the tool result
                            follow_up_response = agent.process_turn(
                                user_input="",  # Empty input since we're continuing
                                conversation_history=self.conversation_history,
                                stream=True
                            )

                            # Process follow-up chunks
                            for follow_chunk in follow_up_response:
                                if follow_chunk.content:
                                    print(follow_chunk.content, end="", flush=True)
                                    accumulated_content += follow_chunk.content

                        except Exception as tool_error:
                            logger.error(f"Tool execution failed: {tool_error}")
                            print(f"\n[å·¥å…·æ‰§è¡Œå¤±è´¥: {tool_call.name}]", end="", flush=True)

            # Ensure we end with a newline
            if accumulated_content:
                print()

            # Add user and assistant messages to history
            self.conversation_history.append(Message(role=Role.USER, content=user_input))
            if accumulated_content:
                self.conversation_history.append(Message(
                    role=Role.ASSISTANT,
                    content=accumulated_content
                ))

            # Log the conversation
            from agent_core import RouteDecision
            dummy_route_decision = RouteDecision(
                engine="chat",
                reason="Streaming mode with tool calls",
                confidence=1.0
            )
            self.logger.log_turn(
                route_decision=dummy_route_decision,
                messages=self.conversation_history[-2:],
                tool_calls_count=tool_calls_count,
                ask_cycles_used=0
            )

        except Exception as e:
            print(f"\nç›´æ¥æµå¼è¾“å‡ºå¤„ç†å‡ºé”™: {e}")
            logger.error(f"Error in direct streaming: {e}")
            # Fallback to non-streaming
            initial_state = create_initial_state(user_input)
            self._handle_graph_execution(initial_state)

    def _handle_planner_execution(self, user_input: str, route_decision):
        """Handle planner execution using LangGraph planner."""
        try:
            # Create initial state for planner
            initial_state = create_initial_state(user_input)

            # Execute planner graph
            config = {"configurable": {"thread_id": f"planner-{user_input[:20]}"}}  # Use input prefix for thread ID
            final_state = planner_app.invoke(initial_state, config=config)

            # Extract the final answer
            if final_state["final_answer"]:
                print(final_state["final_answer"])
            elif final_state["messages"]:
                # Get the last assistant message
                last_msg = final_state["messages"][-1]
                if hasattr(last_msg, 'content') and last_msg.content:
                    print(last_msg.content)

            # Update conversation history
            if final_state.get("messages"):
                self.conversation_history = final_state["messages"]

            # Log the conversation
            self.logger.log_turn(
                route_decision=route_decision,
                messages=final_state.get("messages", []),
                tool_calls_count=final_state.get("tool_call_count", 0),
                ask_cycles_used=0  # Planner handles ask cycles internally
            )

        except Exception as e:
            print(f"\nè§„åˆ’æ‰§è¡Œå‡ºé”™: {e}")
            logger.error(f"Error in planner execution: {e}")
            # Fallback to direct streaming
            self._handle_direct_streaming(user_input)

    def _handle_regular_response(self, result):
        """Handle regular (non-streaming) response."""
        # Update conversation history
        self.conversation_history = result["final_messages"]

        # Log the conversation turn
        self.logger.log_turn(
            route_decision=result["route_decision"],
            messages=result["final_messages"],
            tool_calls_count=result["tool_calls_made"],
            ask_cycles_used=result["ask_cycles_used"]
        )

        # Handle response
        if result.get("awaiting_user_input"):
            self.awaiting_user_input = True
            print(f"\r{result['response']}")
        else:
            self.awaiting_user_input = False
            self._display_response(result)

    def _handle_streaming_response(self, result_generator, user_input: str):
        """Handle streaming response."""
        accumulated_content = ""
        tool_calls_made = 0
        ask_cycles_used = 0

        try:
            for chunk in result_generator:
                # Print content chunks
                if chunk.content:
                    print(chunk.content, end="", flush=True)
                    accumulated_content += chunk.content

                # Handle tool calls
                if chunk.tool_calls:
                    tool_calls_made += len(chunk.tool_calls)

                # Handle finish
                if chunk.finish_reason:
                    if chunk.finish_reason == "stop":
                        print()  # New line at the end
                    elif chunk.finish_reason == "error":
                        print(f"\næµå¼è¾“å‡ºå‡ºé”™: {chunk.content}")
                        return

            # Create a synthetic result for logging
            route_decision = RouteDecision(
                engine="streaming",
                reason="streaming_response",
                confidence=1.0
            )

            # Log the conversation turn
            self.logger.log_turn(
                route_decision=route_decision,
                messages=[
                    Message(role=Role.USER, content=user_input),
                    Message(role=Role.ASSISTANT, content=accumulated_content)
                ],
                tool_calls_count=tool_calls_made,
                ask_cycles_used=ask_cycles_used
            )

            # Update conversation history
            self.conversation_history = [
                Message(role=Role.USER, content=user_input),
                Message(role=Role.ASSISTANT, content=accumulated_content)
            ]

        except Exception as e:
            print(f"\næµå¼è¾“å‡ºå¤„ç†å‡ºé”™: {e}")
            logger.error(f"Error handling streaming response: {e}")

    def _show_graph_status(self):
        """Show current graph execution status."""
        print("\n=== LangGraph æ‰§è¡ŒçŠ¶æ€ ===")
        print(f"å½“å‰èŠ‚ç‚¹: user_input (ç­‰å¾…ç”¨æˆ·è¾“å…¥)")
        print(f"å¯¹è¯è½®æ•°: {len([msg for msg in self.conversation_history if msg.role == Role.USER])}")
        print(f"æ¶ˆæ¯æ€»æ•°: {len(self.conversation_history)}")
        print(f"ç­‰å¾…ç”¨æˆ·è¾“å…¥: {getattr(self, 'awaiting_user_input', False)}")
        print(f"æµå¼è¾“å‡º: {self.stream}")
        print("æ‰§è¡Œè·¯å¾„: user_input â†’ decide_route â†’ model_call â†’ [tool_exec|need_user|end]")
        print("=" * 30)

    def _show_route_status(self):
        """Show current routing decision."""
        print("\n=== è·¯ç”±å†³ç­–çŠ¶æ€ ===")
        print("æ”¯æŒçš„è·¯ç”±æ¨¡å¼:")
        print("  â€¢ chat: ç®€å•å¯¹è¯å’ŒåŸºç¡€å·¥å…·è°ƒç”¨")
        print("  â€¢ reasoner: å¤æ‚æ¨ç†å’Œæ•°å­¦è®¡ç®—")
        print("  â€¢ planner: å¤æ‚è§„åˆ’å’Œå¤šæ­¥éª¤ä»»åŠ¡")
        print("  â€¢ auto_rag: è‡ªåŠ¨çŸ¥è¯†æ‰©å……")
        print("\nå†³ç­–ä¾æ®:")
        print("  â€¢ å…³é”®è¯æ£€æµ‹ï¼ˆè®¡åˆ’ã€æ–¹æ¡ˆã€å¤šæ­¥éª¤ç­‰ï¼‰")
        print("  â€¢ æ–‡æœ¬é•¿åº¦é˜ˆå€¼ï¼ˆ>100å­—ç¬¦å€¾å‘è§„åˆ’ï¼‰")
        print("  â€¢ ç»“æ„åŒ–æ¨¡å¼ï¼ˆæ•°å­—åˆ—è¡¨ã€æµç¨‹å›¾ç­‰ï¼‰")
        print("  â€¢ å¤æ‚åº¦è¯„ä¼°")
        print("=" * 30)

    def _show_prompt_status(self, command: str):
        """Show current system prompt and tools."""
        parts = command.split()
        show_system = len(parts) <= 1 or 'system' in parts[1:]
        show_tools = len(parts) <= 1 or 'tools' in parts[1:]

        print("\n=== ç³»ç»Ÿæç¤ºçŠ¶æ€ ===")

        if show_system:
            print("\nğŸ“ ç³»ç»Ÿæç¤º:")
            try:
                from tool_prompt_builder import load_system_prompt
                system_prompt = load_system_prompt("chat")  # Default to chat
                # Show first 200 chars and indicate if truncated
                if len(system_prompt) > 200:
                    print(f"{system_prompt[:200]}...")
                    print(f"\n(å®Œæ•´æç¤ºé•¿åº¦: {len(system_prompt)} å­—ç¬¦)")
                else:
                    print(system_prompt)
            except Exception as e:
                print(f"åŠ è½½ç³»ç»Ÿæç¤ºå¤±è´¥: {e}")

        if show_tools:
            print("\nğŸ› ï¸ å·¥å…·æ¸…å•:")
            try:
                from tool_prompt_builder import build_tool_prompts
                tool_prompts = build_tool_prompts()
                print(f"å·²æ³¨å†Œå·¥å…·æ•°é‡: {len(tool_prompts['tools'])}")
                for tool in tool_prompts["tool_name_index"]:
                    desc = tool_prompts["tool_name_index"][tool]["description"]
                    print(f"  â€¢ {tool}: {desc[:50]}{'...' if len(desc) > 50 else ''}")
            except Exception as e:
                print(f"åŠ è½½å·¥å…·æ¸…å•å¤±è´¥: {e}")

        print("=" * 30)

    def _show_plan_status(self):
        """Show current plan details."""
        print("\n=== å½“å‰è®¡åˆ’è¯¦æƒ… ===")

        # This would ideally access the current graph state
        # For now, show general information
        print("Planner v1.0: æ”¯æŒç»“æ„åŒ–ä»»åŠ¡è§„åˆ’ä¸æ‰§è¡Œ")
        print("è®¡åˆ’ç”Ÿæˆ: Reasoneræ¨¡å‹ + JSONæ¨¡å¼")
        print("æ‰§è¡Œåˆ†å‘: æ™ºèƒ½è·¯ç”±åˆ°Chat/Reasoneræ‰§è¡Œè€…")
        print("å·¥å…·è°ƒç”¨: ä¸¥æ ¼ä¸¤æ­¥åè®® (tool_calls â†’ role:tool â†’ ç»§ç»­)")
        print("\nè®¡åˆ’åŒ…å«å­—æ®µ:")
        print("  â€¢ id: å”¯ä¸€æ ‡è¯†ç¬¦")
        print("  â€¢ title: ä»»åŠ¡æ ‡é¢˜")
        print("  â€¢ type: tool/chat/reason/write/research")
        print("  â€¢ executor: auto/chat/reasoner")
        print("  â€¢ tool: å·¥å…·åç§° (type=toolæ—¶)")
        print("  â€¢ input: è¾“å…¥å‚æ•°")
        print("  â€¢ expected_output: æœŸæœ›è¾“å‡º")
        print("  â€¢ needs: ç¼ºå¤±ä¿¡æ¯éœ€æ±‚")
        print("\næ‰§è¡Œè€…é€‰æ‹©ä¼˜å…ˆçº§:")
        print("  1. TodoItem.executor (æ˜¾å¼æŒ‡å®š)")
        print("  2. TOOL_META.executor_default")
        print("  3. å¤æ‚åº¦è‡ªåŠ¨åˆ¤æ–­")
        print("  4. Chat (é»˜è®¤)")
        print("=" * 30)

    def _show_todo_details(self, todo_id: str):
        """Show details for a specific todo item."""
        print(f"\n=== TODO {todo_id} è¯¦æƒ… ===")

        # This would access the current plan from graph state
        # For now, show placeholder information
        print(f"TODO ID: {todo_id}")
        print("çŠ¶æ€: è®¡åˆ’ä¸­ (å®é™…è¿è¡Œæ—¶ä¼šæ˜¾ç¤ºæ‰§è¡ŒçŠ¶æ€)")
        print("\nå­—æ®µè¯´æ˜:")
        print("  â€¢ title: ä»»åŠ¡å…·ä½“æè¿°")
        print("  â€¢ type: æ‰§è¡Œç±»å‹ (tool/chat/reason/write/research)")
        print("  â€¢ executor: æ‰§è¡Œè€… (chat/reasoner)")
        print("  â€¢ tool: å·¥å…·åç§° (type=toolæ—¶)")
        print("  â€¢ input: è¾“å…¥å‚æ•°")
        print("  â€¢ expected_output: æœŸæœ›äº§å‡ºæ ¼å¼")
        print("  â€¢ output: å®é™…æ‰§è¡Œç»“æœ")
        print("  â€¢ needs: ç¼ºå¤±ä¿¡æ¯éœ€æ±‚")
        print("\nåœ¨å®é™…è¿è¡Œä¸­ï¼Œæ­¤å‘½ä»¤ä¼šæ˜¾ç¤ºè¯¥TODOçš„å®Œæ•´çŠ¶æ€ä¿¡æ¯ã€‚")
        print("=" * 30)

    def _handle_clarification(self, clarification: str):
        """Handle user clarification after AskUser."""
        print("æ­£åœ¨ç»§ç»­å¤„ç†...", end="", flush=True)

        try:
            # Get the last route decision from history
            # In a more sophisticated implementation, we'd store this in context
            # For now, we'll re-route based on the clarification
            from .agent_core import RouteDecision

            # Simple heuristic: check if clarification is complex
            route_decision = agent.router.route(clarification, None)

            # Continue with clarification
            result = agent.continue_with_user_clarification(
                clarification,
                self.conversation_history,
                route_decision
            )

            # Update conversation history
            self.conversation_history = result["final_messages"]

            # Log continuation
            self.logger.log_continuation(
                clarification=clarification,
                messages=result["final_messages"],
                tool_calls_count=result["tool_calls_made"]
            )

            # Reset state and display response
            self.awaiting_user_input = False
            self._display_response(result)

        except Exception as e:
            print(f"\rå¤„ç†æ¾„æ¸…ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            logger.error(f"Error handling clarification: {e}")
            self.awaiting_user_input = False

    def _display_response(self, result: dict):
        """Display the final response."""
        response = result["response"]

        if self.stream:
            # Simulate streaming by printing character by character
            print("\r", end="")
            for char in response:
                print(char, end="", flush=True)
                # Small delay for streaming effect (optional)
        else:
            print(f"\r{response}")

        # Show statistics if available
        tool_calls = result.get("tool_calls_made", 0)
        if tool_calls > 0:
            print(f"\n[ä½¿ç”¨äº† {tool_calls} ä¸ªå·¥å…·è°ƒç”¨]")

    def cleanup(self):
        """Cleanup resources."""
        if hasattr(self.logger, 'close'):
            self.logger.close()


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="èµ›åšå…‹éš†AIåŠ©æ‰‹ CLI")
    parser.add_argument(
        "--stream",
        action="store_true",
        help="å¯ç”¨æµå¼è¾“å‡º"
    )

    args = parser.parse_args()

    app = CLIApp(stream=args.stream)
    try:
        app.run()
    finally:
        app.cleanup()


if __name__ == "__main__":
    main()
