"""
Node definitions for the DigitalClone AI Assistant LangGraph.

This module contains all the node functions that make up the execution graph.
"""

import logging
import json
import re
from typing import Dict, Any, List, Optional

import sys
import os
from pathlib import Path

# Add the project root to Python path for proper imports
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from langchain_core.messages import HumanMessage, AIMessage, ToolMessage

# Import state types first (always needed)
from .state import AgentState, Route, TodoItem, TodoType

# Conditional imports to support both relative and absolute imports
try:
    from backend.message_types import Message, Role, RouteDecision, ToolCall
    from backend.agent_core import AgentRouter
    from backend.llm_interface import create_llm_client
    from backend.tool_registry import registry
    from backend.tool_prompt_builder import build_tool_prompts, load_system_prompt
    from backend.config import config
    from backend.logger import ConversationLogger
    from backend.ask_user_policy import needs_user_clarification
except ImportError:
    # Fallback to relative imports if absolute imports fail
    from message_types import Message, Role, RouteDecision, ToolCall
    from agent_core import AgentRouter
    from llm_interface import create_llm_client
    from tool_registry import registry
    from tool_prompt_builder import build_tool_prompts, load_system_prompt
    from config import config
    from logger import ConversationLogger

logger = logging.getLogger(__name__)


def user_input_node(state: AgentState) -> Dict[str, Any]:
    """
    User input processing node.

    This node receives user input and prepares the initial state.
    In our implementation, this is handled by create_initial_state,
    so this node mainly serves as an entry point.
    """
    logger.info(f"Processing user input: {state['messages'][-1].content[:50]}...")

    # Update execution path
    state["execution_path"].append("user_input")
    state["current_node"] = "user_input"

    # Return unchanged state - routing will happen next
    return state


def decide_route_node(state: AgentState) -> Dict[str, Any]:
    """
    Route decision node.

    Decides whether to use chat or reasoner model based on the input.
    """
    logger.info("Making routing decision...")

    if not state["messages"]:
        raise ValueError("No messages in state")

    user_message = None
    for msg in reversed(state["messages"]):
        if msg.role == Role.USER:
            user_message = msg
            break

    if not user_message:
        raise ValueError("No user message found")

    # Use existing router logic
    router = AgentRouter()
    route_decision = router.route(user_message.content, None)  # Simplified context

    # Map to our Route enum
    if route_decision.engine == "reasoner":
        route = Route.REASONER
    else:
        route = Route.CHAT

    logger.info(f"Routing decision: {route.value} (reason: {route_decision.reason})")

    # Update state
    state["route"] = route
    state["route_decision"] = route_decision
    state["execution_path"].append("decide_route")
    state["current_node"] = "decide_route"

    return state


def model_call_node(state: AgentState) -> Dict[str, Any]:
    """
    Model call node.

    Calls the appropriate LLM (chat or reasoner) with tools and system prompts.
    """
    logger.info(f"Making model call with route: {state['route'].value}")

    route = state["route"]
    messages = state["messages"]

    # Load appropriate system prompt
    try:
        from backend.tool_prompt_builder import load_system_prompt
    except ImportError:
        from tool_prompt_builder import load_system_prompt
    system_prompt = load_system_prompt(route.value)

    # Select appropriate LLM client
    if route == Route.REASONER:
        llm_client = create_llm_client("reasoner")
    else:
        llm_client = create_llm_client("chat")

    # Build tool prompts
    tool_prompts = build_tool_prompts()
    functions = tool_prompts["tools"]

    # For now, we'll handle streaming separately in CLI
    # This node focuses on the core logic
    response = llm_client.generate(
        messages,
        functions=functions,
        system_prompt=system_prompt,
        stream=False
    )

    # Handle response
    if response.tool_calls:
        # Has tool calls - store for execution
        tool_call = response.tool_calls[0]
        state["pending_tool_call"] = {
            "name": tool_call.name,
            "arguments": tool_call.arguments,
            "id": getattr(tool_call, 'id', None)  # May not have ID yet
        }
        logger.info(f"Tool call detected: {tool_call.name}")

        # Add assistant message to conversation
        assistant_message = Message(
            role=Role.ASSISTANT,
            content=response.content,
            tool_call=tool_call
        )
        state["messages"].append(assistant_message)

    elif needs_user_clarification(response.content):
        # Needs user clarification
        state["awaiting_user"] = True
        state["user_input_buffer"] = None

        # Add assistant message
        assistant_message = Message(
            role=Role.ASSISTANT,
            content=response.content
        )
        state["messages"].append(assistant_message)

        logger.info("AskUser clarification needed")

    else:
        # Final answer
        state["final_answer"] = response.content
        state["should_end"] = True

        # Add assistant message
        assistant_message = Message(
            role=Role.ASSISTANT,
            content=response.content
        )
        state["messages"].append(assistant_message)

        logger.info("Final answer generated")

    state["execution_path"].append("model_call")
    state["current_node"] = "model_call"

    return state


def tool_exec_node(state: AgentState) -> Dict[str, Any]:
    """
    Tool execution node with executor routing and two-step protocol.

    Uses the specified executor to call tools with proper LLM interaction.
    """
    logger.info("Executing tool with executor routing...")

    if not state["pending_tool_call"]:
        raise ValueError("No pending tool call in state")

    tool_call_info = state["pending_tool_call"]
    tool_name = tool_call_info["tool"]
    input_data = tool_call_info["input_data"]
    todo_id = tool_call_info["todo_id"]
    executor = tool_call_info["executor"]

    logger.info(f"âš™ï¸ TOOL EXECUTION START - {tool_name} with executor: {executor}")
    logger.info(f"   Todo ID: {todo_id}")
    logger.info(f"   Input Data: {input_data}")

    # Find the corresponding todo item
    current_todo = None
    if state["plan"]:
        for todo in state["plan"]:
            if todo.id == todo_id:
                current_todo = todo
                break

    if not current_todo:
        logger.error(f"âŒ Could not find todo with id {todo_id}")
        state["pending_tool_call"] = None
        return state

    # Execute tool using the two-step protocol
    task_context = f"{current_todo.title}\nç›®æ ‡ï¼š{current_todo.expected_output}"
    logger.info(f"ğŸ”„ Calling tool with context: {task_context}")

    tool_result = call_tool_with_llm(executor, tool_name, task_context, state)

    if tool_result["success"]:
        # Store tool result for potential reflective replanning
        state["last_tool_result"] = tool_result

        # Update todo with result
        current_todo.output = tool_result["summary"]
        logger.info(f"âœ… TOOL EXECUTION SUCCESS - {tool_name}")
        logger.info(f"   Result: {tool_result['summary'][:200]}{'...' if len(tool_result['summary']) > 200 else ''}")

        # Check if tool result contains content to add to chat context
        if "value" in tool_result and isinstance(tool_result["value"], dict):
            tool_value = tool_result["value"]
            if tool_value.get("add_to_context") and "content" in tool_value:
                # Add file content to chat context
                content = tool_value["content"]
                file_path = tool_value.get("file_path", "unknown file")

                context_message = Message(
                    role=Role.SYSTEM,
                    content=f"å·²è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆ{file_path}ï¼‰ï¼š\n\n{content[:2000]}{'...' if len(content) > 2000 else ''}\n\nè¯·åŸºäºä»¥ä¸Šæ–‡ä»¶å†…å®¹å›ç­”åç»­é—®é¢˜ã€‚"
                )
                state["messages"].append(context_message)
                logger.info(f"ğŸ“ Added file content to chat context ({len(content)} chars)")

        # Advance to next todo
        current_idx = state.get("current_todo", 0)
        state["current_todo"] = current_idx + 1
        logger.info(f"â­ï¸ ADVANCING to next todo (index: {current_idx + 1})")

    else:
        if "needs_clarification" in tool_result:
            # Trigger ask_user_interrupt - pause the tool call for later resumption
            state["paused_tool_call"] = state["pending_tool_call"]  # Save for resumption
            state["pending_tool_call"] = None  # Clear current pending call
            state["awaiting_user"] = True
            state["user_input_buffer"] = tool_result["needs_clarification"]
            logger.info(f"â¸ï¸ TOOL EXECUTION PAUSED - Needs user clarification:")
            logger.info(f"   Question: {tool_result['needs_clarification']}")
            return state
        else:
            # Tool execution failed - add error to conversation for replanning
            error_msg = f"å·¥å…·æ‰§è¡Œå¤±è´¥ï¼š{tool_result['error']}"
            current_todo.output = error_msg
            logger.error(f"âŒ TOOL EXECUTION FAILED - {tool_name}: {tool_result['error']}")

            # Add error message to conversation so Chat model can see it
            from backend.message_types import Message, Role
            error_message = Message(
                role=Role.SYSTEM,
                content=f"å·¥å…·è°ƒç”¨å¤±è´¥åé¦ˆï¼š{tool_name}æ‰§è¡Œå¤±è´¥ï¼Œé”™è¯¯ï¼š{tool_result['error']}ã€‚è¯·é‡æ–°è§„åˆ’æˆ–æä¾›æ›¿ä»£æ–¹æ¡ˆã€‚"
            )
            state["messages"].append(error_message)
            logger.info("ğŸ“ Added tool failure feedback to conversation")

            # Trigger replanning by going back to planner
            logger.info("ğŸ”„ TRIGGERING REPLAN due to tool failure")
            state["current_node"] = "planner_generate"
            state["pending_tool_call"] = None
            return state

    # Clear pending tool call
    state["pending_tool_call"] = None
    logger.info("ğŸ§¹ Cleared pending tool call")

    state["execution_path"].append("tool_exec")
    state["current_node"] = "tool_exec"

    return state


def need_user_node(state: AgentState) -> Dict[str, Any]:
    """
    Check if user clarification is needed.

    This is a conditional node that determines the flow.
    """
    needs_user = state["awaiting_user"]
    logger.info(f"User clarification needed: {needs_user}")

    state["execution_path"].append("need_user")
    state["current_node"] = "need_user"

    return state


def ask_user_interrupt_node(state: AgentState) -> Dict[str, Any]:
    """
    Human-in-the-loop interrupt node.

    This node handles resumption after user clarification or parameter input.
    It can handle both paused tool calls and missing parameter collection.
    """
    logger.info("ğŸ‘¤ USER INTERACTION COMPLETE - Resuming execution")

    # Check if there's user input to process (from CLI parameter collection)
    if state.get("user_provided_input") and state.get("needs_info"):
        user_input = state["user_provided_input"]
        needs_info = state["needs_info"]

        logger.info(f"ğŸ“ PROCESSING user input from CLI: {user_input}")

        # Find and update the corresponding todo
        todo_id = needs_info["todo_id"]
        for todo in state["plan"]:
            if todo.id == todo_id:
                if not todo.input_data:
                    todo.input_data = {}

                # Update todo with user input
                for param, value in user_input.items():
                    todo.input_data[param] = value
                    logger.info(f"   Updated {param}: {value}")

                logger.info(f"âœ… Todo {todo.id} parameters updated, ready for execution")
                break

        # Clear the user input state
        state.pop("user_provided_input", None)
        state.pop("needs_info", None)

    # Check if there's a paused tool call to resume
    elif state.get("paused_tool_call"):
        # Resume the paused tool call
        state["pending_tool_call"] = state["paused_tool_call"]
        state["paused_tool_call"] = None
        logger.info("â–¶ï¸ RESUMING paused tool call after user input")
        logger.info(f"   Tool: {state['pending_tool_call']['tool']}")
        logger.info(f"   Todo: {state['pending_tool_call']['todo_id']}")

    # Clear clarification state
    state["awaiting_user"] = False
    state["user_input_buffer"] = None
    logger.info("ğŸ§¹ Cleared user interaction state")

    state["execution_path"].append("ask_user_interrupt")
    state["current_node"] = "ask_user_interrupt"

    return state


def end_node(state: AgentState) -> Dict[str, Any]:
    """
    End node for successful completion.
    """
    logger.info("Conversation completed successfully")

    state["execution_path"].append("end")
    state["current_node"] = "end"
    state["should_end"] = True

    return state


# ===== PLANNER NODES =====

def classify_intent_node(state: AgentState) -> Dict[str, Any]:
    """
    Classify user intent and choose execution pipeline.

    Routes to: chat, planner, auto_rag
    Locks planner route until completion.
    """
    logger.info("Classifying user intent...")

    # Check if we have user input to process (continuation from CLI)
    if state.get("user_provided_input"):
        logger.info("Detected user input continuation, routing to ask_user_interrupt")
        state["execution_path"].append("classify_intent")
        state["current_node"] = "ask_user_interrupt"
        # Set a flag to indicate we should route to ask_user_interrupt
        state["_route_to_ask_user"] = True
        return state

    # Check if route is already locked (from previous planner execution)
    if state.get("route_locked", False):
        logger.info("Route locked, continuing with planner")
        state["execution_path"].append("classify_intent")
        state["current_node"] = "classify_intent"
        return state

    if not state["messages"]:
        raise ValueError("No messages in state")

    user_message = None
    for msg in reversed(state["messages"]):
        if msg.role.value == "user":
            user_message = msg
            break

    if not user_message:
        raise ValueError("No user message found")

    user_input = user_message.content.lower()

    # Check for explicit auto_rag triggers
    if any(keyword in user_input for keyword in ['è‡ªåŠ¨æ‰©å……', 'æ•´ç†å¯¹è¯', 'æ€»ç»“å¯¹è¯', 'auto_rag']):
        route = Route.AUTO_RAG
        reason = "ç”¨æˆ·æ˜ç¡®è¯·æ±‚è‡ªåŠ¨çŸ¥è¯†æ‰©å……"
    # Check for planning keywords (must match AgentRouter.COMPLEX_KEYWORDS)
    elif any(keyword in user_input for keyword in [
        'è®¡åˆ’', 'è§„åˆ’', 'åˆ¶å®š', 'åˆ†è§£', 'å¤šæ­¥éª¤', 'è°ƒç ”', 'å†™æ–¹æ¡ˆ', 'è¯„ä¼°', 'å¯¹æ¯”',
        'æµç¨‹', 'ä¾èµ–', 'é‡Œç¨‹ç¢‘', 'roadmap', 'strategy', 'systematic',
        'complex', 'comprehensive', 'detailed', 'step-by-step', 'breakdown',
        'åˆ†æ', 'æ€»ç»“', 'æŠ¥å‘Š', 'æŸ¥æ‰¾', 'æœç´¢', 'ç ”ç©¶', 'è°ƒæŸ¥', 'æ•´ç†',
        'ç»¼åˆ', 'æ•´åˆ', 'æ¯”è¾ƒ', 'è¯„ä¼°', 'æ’°å†™', 'ç”Ÿæˆ', 'åˆ›å»º'
    ]) or len(user_input) > 100:  # Long inputs likely need planning
        route = Route.PLANNER
        reason = "æ£€æµ‹åˆ°å¤æ‚è§„åˆ’ä»»åŠ¡ç‰¹å¾"
        # Lock the route for planner execution
        state["route_locked"] = True
    else:
        route = Route.CHAT
        reason = "ç®€å•å¯¹è¯ä»»åŠ¡ï¼Œä½¿ç”¨chatæ¨¡å‹"

    logger.info(f"Intent classification: {route.value} ({reason})")

    state["route"] = route
    state["execution_path"].append("classify_intent")
    state["current_node"] = "classify_intent"

    return state


def sufficiency_check_node(state: AgentState) -> Dict[str, Any]:
    """
    Check if we have sufficient information to proceed with planning.

    This is a simplified version - in production, you'd use more sophisticated analysis.
    """
    logger.info("Checking information sufficiency...")

    # For now, assume we have enough information
    # In production, this would analyze the plan and check for missing prerequisites
    state["sufficiency"] = "enough"
    state["execution_path"].append("sufficiency_check")
    state["current_node"] = "sufficiency_check"

    return state


def planner_generate_node(state: AgentState) -> Dict[str, Any]:
    """
    Generate a structured plan using the new three-phase approach:
    1. Chat model creates quick draft (<100 words)
    2. Reasoner model reviews and improves (<200 word prompt)
    3. Chat model outputs final JSON plan with execution dependencies

    This ensures better planning quality while avoiding Reasoner instability.
    """
    logger.info("Generating structured plan using three-phase approach...")

    if not state["messages"]:
        raise ValueError("No messages in state")

    # Get the user request
    user_request = ""
    for msg in state["messages"]:
        if msg.role.value == "user":
            user_request = msg.content
            break

    try:
        # Phase 1: Chat model creates quick draft (<100 words)
        logger.info("ğŸ“ Phase 1: Chat model creating quick draft...")
        chat_client = create_llm_client("chat")

        # Get available tools summary for context
        tools_summary = "å¯ç”¨å·¥å…·ï¼šæ–‡ä»¶è¯»å–(file_read)ã€ç½‘ç»œæœç´¢(web_search)ã€è®¡ç®—å™¨(calculator)ã€æ–‡æ¡£å†™å…¥(markdown_writer)ã€RAGæœç´¢(rag_search)ç­‰ã€‚"

        quick_draft_prompt = f"""ç”¨æˆ·ä»»åŠ¡ï¼š{user_request}

ç”¨<100å­—è§„åˆ’æ‰§è¡Œæ–¹æ¡ˆï¼š
1. æ­¥éª¤åºåˆ—ï¼ˆä¸²è¡Œ/å¹¶è¡Œæ ‡æ³¨ï¼‰
2. å„æ­¥éª¤å·¥å…·åŠå‚æ•°éœ€æ±‚
3. ä¾èµ–å…³ç³»
4. ç”¨æˆ·éœ€æä¾›çš„å‚æ•°

{tools_summary}

æ ¼å¼ï¼šæ­¥éª¤N(ä¸²è¡Œ/å¹¶è¡Œ)ï¼šå·¥å…·(å‚æ•°=æ¥æº) â†’ ç»“æœ"""

        # Manage conversation history - compress if too long
        total_chars = sum(len(str(msg.content or "")) for msg in state["messages"])
        # Estimate tokens (rough approximation: 1 token â‰ˆ 4 chars)
        estimated_tokens = total_chars // 4
        if estimated_tokens > 8000:  # 8k tokens limit
            logger.info(f"ğŸ“š Compressing conversation history: ~{estimated_tokens} tokens -> summarizing")
            state["messages"] = _compress_conversation_history(state["messages"])

        try:
            quick_response = chat_client.generate(
                messages=[Message(role=Role.USER, content=quick_draft_prompt)],
                stream=False
            )
            draft_plan = quick_response.content.strip()
            logger.info(f"ğŸ“ PHASE 1 COMPLETE - Quick draft ({len(draft_plan)} chars):")
            logger.info(f"   Draft: {draft_plan}")
        except Exception as e:
            logger.warning(f"âŒ Phase 1 failed: {e}, using fallback plan")
            draft_plan = f"åˆ†æç”¨æˆ·éœ€æ±‚ï¼š{user_request[:50]}... ä½¿ç”¨ç›¸å…³å·¥å…·è·å–ä¿¡æ¯å¹¶ç”Ÿæˆç»“æœã€‚éœ€è¦æ–‡ä»¶è¯»å–ã€ç½‘ç»œæœç´¢ã€æ–‡æ¡£å†™å…¥ç­‰å·¥å…·ã€‚"
            logger.info(f"ğŸ“ PHASE 1 FALLBACK - Draft: {draft_plan}")

        # Phase 2: Reasoner model reviews the draft (<200 word prompt)
        logger.info("ğŸ¤” PHASE 2 START - Reasoner model reviewing draft...")

        # Get available tools for context
        try:
            from backend.tool_prompt_builder import build_tool_prompts
            tool_prompts = build_tool_prompts()
            available_tools = tool_prompts.get("tool_name_index", {})
            tools_list = list(available_tools.keys())
            tools_summary = ', '.join(tools_list)
        except Exception as e:
            logger.warning(f"Could not load tools for planning review: {e}")
            tools_summary = "file_read, web_search, calculator, markdown_writer, rag_search"

        # Fixed reasoner prompt template (<200 words)
        reasoner_review_template = f"""å®¡æŸ¥æ‰§è¡Œæ–¹æ¡ˆï¼š

ç”¨æˆ·éœ€æ±‚ï¼š{{user_request}}

æ–¹æ¡ˆï¼š{{draft_plan}}

å·¥å…·å‚æ•°ï¼š
{tools_summary}

é‡è¦è¯´æ˜ï¼šä½ åªèƒ½å»ºè®®ä½¿ç”¨ä¸Šè¿°åˆ—è¡¨ä¸­çš„å·¥å…·ï¼Œä¸èƒ½å‘æ˜æˆ–å‡è®¾ä¸å­˜åœ¨çš„å·¥å…·ï¼

åˆ†æ(<80å­—)ï¼š
- æµç¨‹åˆç†æ€§ï¼Ÿ
- å‚æ•°å®Œæ•´æ€§ï¼Ÿ
- ç”¨æˆ·éœ€æä¾›å“ªäº›å‚æ•°ï¼Ÿ

è¾“å‡ºæ”¹è¿›å»ºè®®ã€‚"""

        # Don't truncate inputs too aggressively - let the model handle longer context
        reasoner_prompt = reasoner_review_template.format(
            user_request=user_request,
            draft_plan=draft_plan
        )

        logger.info(f"ğŸ¤” PHASE 2 PROMPT ({len(reasoner_prompt)} chars):")
        # Only log first 500 chars to avoid log spam, but don't truncate the actual prompt
        logger.info(f"   Prompt: {reasoner_prompt[:500]}{'...' if len(reasoner_prompt) > 500 else ''}")

        # Remove hard truncation - let the model handle the full context
        # The 200 token limit is a soft guideline, not a hard requirement

        try:
            reasoner_client = create_llm_client("reasoner")
            review_response = reasoner_client.generate(
                messages=[Message(role=Role.USER, content=reasoner_prompt)],
                stream=False
            )
            review_feedback = review_response.content.strip()
            logger.info(f"ğŸ¤” PHASE 2 COMPLETE - Reasoner review ({len(review_feedback)} chars):")
            logger.info(f"   Review: {review_feedback}")
        except Exception as e:
            logger.warning(f"âŒ Phase 2 failed: {e}, skipping review")
            review_feedback = "æ–¹æ¡ˆåŸºæœ¬åˆç†ï¼Œå¯ä»¥æŒ‰åŸè®¡åˆ’æ‰§è¡Œã€‚ç¡®ä¿åŒºåˆ†ä¸²è¡Œå’Œå¹¶è¡Œä»»åŠ¡ã€‚"
            logger.info(f"ğŸ¤” PHASE 2 FALLBACK - Review: {review_feedback}")

        # Phase 3: Chat model creates final JSON plan incorporating feedback
        logger.info("ğŸ“‹ PHASE 3 START - Chat model creating final JSON plan...")

        final_planning_prompt = f"""åŸºäºç”¨æˆ·éœ€æ±‚ã€åˆæ­¥æ–¹æ¡ˆå’Œä¸“å®¶åé¦ˆï¼Œåˆ¶å®šæœ€ç»ˆæ‰§è¡Œè®¡åˆ’ã€‚

ç”¨æˆ·éœ€æ±‚ï¼š{user_request}
åˆæ­¥æ–¹æ¡ˆï¼š{draft_plan}
ä¸“å®¶å»ºè®®ï¼š{review_feedback}

ğŸ”´ åŸºäºæ‰§è¡Œæµç¨‹åˆ¶å®šè¯¦ç»†è®¡åˆ’
å‚è€ƒåˆæ­¥æ–¹æ¡ˆä¸­çš„æ­¥éª¤ã€ä¾èµ–å…³ç³»å’Œä¸²å¹¶è¡Œæ§åˆ¶ï¼Œåˆ¶å®šå®Œæ•´çš„æ‰§è¡Œè®¡åˆ’ã€‚

æ³¨æ„ï¼šåªèƒ½ä½¿ç”¨ç³»ç»Ÿå·²æœ‰çš„å·¥å…·ï¼Œä¸è¦å‘æ˜ä¸å­˜åœ¨çš„å·¥å…·ã€‚

è¯·è¾“å‡ºJSONæ ¼å¼çš„è¯¦ç»†æ‰§è¡Œè®¡åˆ’ï¼š

{{
  "goal": "ä»»åŠ¡ç›®æ ‡",
  "success_criteria": "æˆåŠŸæ ‡å‡†",
  "execution_strategy": "serial",
  "todos": [
    {{
      "id": "T1",
      "title": "è¯»å–ç”¨æˆ·æä¾›çš„æ–‡ä»¶å†…å®¹",
      "why": "è·å–æ–‡ä»¶å†…å®¹ä½œä¸ºæŠ¥å‘Šçš„åŸºç¡€",
      "type": "tool",
      "tool": "file_read",
      "executor": "chat",
      "dependencies": [],
      "parallel_group": null,
      "execution_order": 1,
      "input": {{"file_path": "å°†åœ¨æ‰§è¡Œæ—¶ä»ç”¨æˆ·è¾“å…¥è·å–"}},
      "expected_output": "æ–‡ä»¶å†…å®¹çš„æ–‡æœ¬",
      "needs": ["file_path"]
    }},
    {{
      "id": "T2",
      "title": "å°†æ–‡ä»¶å†…å®¹å­˜å…¥RAGç³»ç»Ÿ",
      "why": "ä¸ºåç»­æŸ¥è¯¢å’Œåˆ†æå»ºç«‹çŸ¥è¯†åº“",
      "type": "tool",
      "tool": "rag_upsert",
      "executor": "chat",
      "dependencies": ["T1"],
      "parallel_group": null,
      "execution_order": 2,
      "input": {{"documents": "ä»T1è¯»å–çš„æ–‡ä»¶å†…å®¹"}},
      "expected_output": "æ–‡ä»¶å†…å®¹å·²å­˜å…¥RAGç³»ç»Ÿ",
      "needs": []
    }},
    {{
      "id": "T3",
      "title": "æœç´¢äº’è”ç½‘ç›¸å…³ä¿¡æ¯",
      "why": "è¡¥å……èƒŒæ™¯ä¿¡æ¯",
      "type": "tool",
      "tool": "web_search",
      "executor": "chat",
      "dependencies": ["T1"],
      "parallel_group": null,
      "execution_order": 3,
      "input": {{"query": "åŸºäºæ–‡ä»¶å†…å®¹çš„å…³é”®æœç´¢è¯"}},
      "expected_output": "æœç´¢ç»“æœ",
      "needs": []
    }},
    {{
      "id": "T4",
      "title": "ç”Ÿæˆåˆ†ææŠ¥å‘Š",
      "why": "æ•´åˆæ–‡ä»¶å†…å®¹å’Œæœç´¢ç»“æœç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š",
      "type": "tool",
      "tool": "markdown_writer",
      "executor": "chat",
      "dependencies": ["T1", "T2", "T3"],
      "parallel_group": null,
      "execution_order": 4,
      "input": {{"content": "æ•´åˆçš„æ–‡ä»¶å†…å®¹å’Œæœç´¢ç»“æœ", "filename": "analysis_report"}},
      "expected_output": "ä¿å­˜åˆ°OUTPUT_DIRçš„markdownæ–‡ä»¶",
      "needs": []
    }}
  ]
}}

æ‰§è¡Œæ§åˆ¶è§„èŒƒï¼š
- dependencies: ["T1"] - å¿…é¡»å®Œæˆçš„å‰ç½®ä»»åŠ¡IDæ•°ç»„
- parallel_group: "group1" - åŒç»„ä»»åŠ¡å¯å¹¶è¡Œæ‰§è¡Œï¼ˆç›¸åŒç»„åï¼‰
- execution_order: 1 - ç»„å†…æ‰§è¡Œé¡ºåºï¼ˆä»å°åˆ°å¤§ï¼‰
- needs: ["file_path"] - éœ€è¦ç”¨æˆ·æä¾›çš„å‚æ•°ï¼Œç³»ç»Ÿä¼šä¸­æ–­è¯¢é—®

æ‰§è¡Œè§„åˆ™ï¼š
1. ç›¸åŒparallel_groupçš„ä»»åŠ¡æŒ‰execution_orderé¡ºåºæ‰§è¡Œ
2. ä¸åŒparallel_groupé—´æŒ‰dependencieså…³ç³»ä¸²è¡Œæ‰§è¡Œ
3. æœ‰needså­—æ®µçš„ä»»åŠ¡ä¼šä¸­æ–­æ‰§è¡Œæ”¶é›†ç”¨æˆ·è¾“å…¥
4. markdown_writerè‡ªåŠ¨ä½¿ç”¨OUTPUT_DIRç¯å¢ƒå˜é‡

åªè¾“å‡ºJSONæ ¼å¼ã€‚"""

        logger.info(f"ğŸ“‹ PHASE 3 PROMPT ({len(final_planning_prompt)} chars):")
        logger.info(f"   User Request: {user_request}")
        logger.info(f"   Draft Plan: {draft_plan}")
        logger.info(f"   Review Feedback: {review_feedback}")

        # Manage conversation history for Phase 3 as well
        total_chars = sum(len(str(msg.content or "")) for msg in state["messages"])
        # Estimate tokens (rough approximation: 1 token â‰ˆ 4 chars)
        estimated_tokens = total_chars // 4
        if estimated_tokens > 8000:  # 8k tokens limit
            logger.info(f"ğŸ“š Compressing conversation history for Phase 3: ~{estimated_tokens} tokens")
            state["messages"] = _compress_conversation_history(state["messages"])

        try:
            final_response = chat_client.generate(
                messages=[Message(role=Role.USER, content=final_planning_prompt)],
                stream=False,
                response_format={"type": "json_object"}
            )

            # Parse final JSON plan
            content = final_response.content.strip()
            logger.info(f"ğŸ“‹ PHASE 3 RESPONSE ({len(content)} chars):")
            logger.info(f"   Raw JSON: {content}")

            try:
                plan_data = json.loads(content)
                logger.info("ğŸ“‹ PHASE 3 COMPLETE - Final JSON plan parsing successful")
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSON parsing failed: {e}")
                # Fallback: try to extract JSON
                import re
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    plan_data = json.loads(json_match.group())
                    logger.info("ğŸ“‹ PHASE 3 COMPLETE - JSON extracted and parsed from fallback")
                else:
                    raise ValueError(f"Failed to parse JSON plan: {content[:200]}...")

        except Exception as e:
            logger.warning(f"Phase 3 failed: {e}, using fallback plan")
            # Create a simple fallback plan
            plan_data = {
                "goal": f"å¤„ç†ç”¨æˆ·è¯·æ±‚ï¼š{user_request[:50]}...",
                "success_criteria": "æˆåŠŸå®Œæˆç”¨æˆ·ä»»åŠ¡",
                "execution_strategy": "serial",
                "todos": [
                    {
                        "id": "T1",
                        "title": "åˆ†æç”¨æˆ·éœ€æ±‚å¹¶æ‰§è¡Œä»»åŠ¡",
                        "why": "ç›´æ¥å“åº”ç”¨æˆ·è¯·æ±‚",
                        "type": "tool",
                        "tool": "file_read",  # Default tool
                        "executor": "chat",
                        "dependencies": [],
                        "parallel_group": None,
                        "execution_order": 1,
                        "input": {"file_path": "éœ€è¦ç”¨æˆ·æŒ‡å®š"},
                "expected_output": "ä»»åŠ¡æ‰§è¡Œç»“æœ",
                "needs": ["file_path"]
                    }
                ]
            }
            logger.info("âœ… Fallback plan created")

        # Validate and convert to TodoItem objects
        required_keys = ["goal", "success_criteria", "todos"]
        for key in required_keys:
            if key not in plan_data:
                logger.warning(f"Missing required key: {key}, using default")
                if key == "goal":
                    plan_data["goal"] = f"å¤„ç†ç”¨æˆ·è¯·æ±‚ï¼š{user_request[:50]}..."
                elif key == "success_criteria":
                    plan_data["success_criteria"] = "æˆåŠŸå®Œæˆç”¨æˆ·ä»»åŠ¡"
                elif key == "todos":
                    plan_data["todos"] = []

        todos = []
        for todo_data in plan_data["todos"]:
            required_todo_keys = ["id", "title", "type"]
            for key in required_todo_keys:
                if key not in todo_data:
                    logger.warning(f"Todo missing required key: {key}, skipping")
                    continue

            todos.append(TodoItem(
                id=todo_data["id"],
                title=todo_data["title"],
                why=todo_data.get("why", ""),
                type=TodoType(todo_data["type"]),
                tool=todo_data.get("tool"),
                executor=todo_data.get("executor", "chat"),
                input_data=todo_data.get("input"),
                dependencies=todo_data.get("dependencies", []),
                parallel_group=todo_data.get("parallel_group"),
                execution_order=todo_data.get("execution_order", 0),
                expected_output=todo_data.get("expected_output", ""),
                needs=todo_data.get("needs", [])
            ))

        state["plan"] = todos
        state["execution_strategy"] = plan_data.get("execution_strategy", "serial")

        logger.info(f"ğŸ¯ FINAL PLAN GENERATED - {len(todos)} todos (strategy: {state['execution_strategy']})")
        logger.info(f"ğŸ“Š Goal: {plan_data.get('goal', 'N/A')}")
        logger.info(f"âœ… Success Criteria: {plan_data.get('success_criteria', 'N/A')}")

        for i, todo in enumerate(todos, 1):
            deps = f" â† {todo.dependencies}" if todo.dependencies else ""
            parallel = f" [å¹¶è¡Œç»„:{todo.parallel_group}]" if todo.parallel_group else ""
            needs = f" [éœ€è¦ç”¨æˆ·è¾“å…¥:{todo.needs}]" if todo.needs else ""
            tool_info = f"[{todo.tool}]" if todo.tool else ""
            executor_info = f"({todo.executor})" if todo.executor else ""
            logger.info(f"   {i}. {todo.id}: {todo.title} {tool_info}{executor_info}{deps}{parallel}{needs}")
            if todo.why:
                logger.info(f"      åŸå› : {todo.why}")
            if todo.expected_output:
                logger.info(f"      é¢„æœŸè¾“å‡º: {todo.expected_output}")

    except Exception as e:
        logger.error(f"Plan generation failed: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        # Create minimal fallback plan
        state["plan"] = [
            TodoItem(
                id="T1",
                title="å¤„ç†ç”¨æˆ·è¯·æ±‚",
                why="æ‰§è¡Œç”¨æˆ·çš„åŸºæœ¬éœ€æ±‚",
                type=TodoType.TOOL,
                tool="file_read",
                executor="chat",
                input_data={"file_path": "éœ€è¦ç”¨æˆ·æŒ‡å®š"},
                dependencies=[],
                parallel_group=None,
                execution_order=1,
                expected_output="ä»»åŠ¡ç»“æœ",
                needs=["file_path"]
            )
        ]
        state["execution_strategy"] = "serial"
        logger.info("âœ… Emergency fallback plan created")

    state["execution_path"].append("planner_generate")
    state["current_node"] = "planner_generate"

    return state


def planner_gate_node(state: AgentState) -> Dict[str, Any]:
    """
    Gate node that checks if we can proceed with plan execution.
    """
    logger.info("Checking planner gate...")

    # Check if we have missing information
    has_missing_info = any(
        todo.needs for todo in state["plan"]
    ) if state["plan"] else False

    if has_missing_info and state["limits"].max_ask_cycles > 0:
        state["sufficiency"] = "missing"
        state["awaiting_user"] = True
    else:
        state["sufficiency"] = "enough"
        state["awaiting_user"] = False

    state["execution_path"].append("planner_gate")
    state["current_node"] = "planner_gate"

    return state


def resolve_executor(todo: TodoItem) -> str:
    """
    Resolve which executor to use for a todo item.

    Returns:
        "chat" or "reasoner"
    """
    # Priority: todo.executor > TOOL_META.executor_default > "chat"
    if todo.executor and todo.executor != "auto":
        return todo.executor

    # Check tool metadata
    if todo.tool:
        tool_meta = registry.get_tool_meta(todo.tool)
        if tool_meta:
            if tool_meta.executor_default == "reasoner":
                return "reasoner"
            if tool_meta.complexity == "complex":
                return "reasoner"

    # Auto-upgrade conditions for chat â†’ reasoner
    input_data = todo.input_data or {}
    input_str = json.dumps(input_data, ensure_ascii=False)

    # Condition 1: Parameter object too large (>512 chars)
    if len(input_str) > 512:
        logger.info(f"Auto-upgrade to reasoner: large parameter object ({len(input_str)} chars)")
        return "reasoner"

    # Condition 2: Previous validation failures (placeholder for future implementation)
    # This would track validation failures and upgrade after N attempts

    # Condition 3: Complex argument construction needed
    if todo.arg_template:
        logger.info(f"Auto-upgrade to reasoner: argument template required")
        return "reasoner"

    # Condition 4: Complex tool types (placeholder for specific tool types)

    # Default to chat
    return "chat"


def truncate_text(text: str, max_chars: int, suffix: str = "...") -> str:
    """
    Truncate text to max_chars, adding suffix if truncated.

    Args:
        text: Text to truncate
        max_chars: Maximum character count
        suffix: Suffix to add if truncated

    Returns:
        Truncated text
    """
    if len(text) <= max_chars:
        return text
    # Ensure we don't go negative
    keep_chars = max(0, max_chars - len(suffix))
    return text[:keep_chars] + suffix




def call_tool_with_llm(executor: str, tool_name: str, task_context: str, state: AgentState) -> Dict[str, Any]:
    """
    Call a tool using the specified executor with proper two-step protocol.

    Returns:
        Dict with result information
    """
    tool_meta = registry.get_tool_meta(tool_name)
    if not tool_meta:
        return {"success": False, "error": f"Tool {tool_name} not found"}

    # Get appropriate system prompt
    system_prompt = load_system_prompt("chat" if executor == "chat" else "reasoner", executor)

    # For Reasoner executor, provide only the specific tool information
    # For Chat executor, provide full context as before
    if executor == "reasoner":
        # Limit all inputs for Reasoner to prevent empty responses
        task_context = truncate_text(task_context, 200)
        tool_desc = truncate_text(tool_meta.description, 150)
        arg_hint = truncate_text(tool_meta.arg_hint or "", 100)

        # Create focused context for Reasoner - only this specific tool
        context_message = f"""ä»»åŠ¡ï¼š{task_context}

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥å®Œæˆä»»åŠ¡ï¼š
- {tool_name}ï¼š{tool_desc}
  å‚æ•°æç¤ºï¼š{arg_hint}

è¯·è°ƒç”¨ {tool_name} å·¥å…·æ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚åªå…è®¸è°ƒç”¨è¿™ä¸ªå·¥å…·å’Œ ask_user å·¥å…·ï¼ˆå¦‚æœéœ€è¦ç”¨æˆ·ä¿¡æ¯ï¼‰ã€‚
"""
        logger.info(f"Reasoner tool execution: task_context={len(task_context)} chars, "
                   f"tool_desc={len(tool_desc)} chars, arg_hint={len(arg_hint)} chars")
    else:
        # Chat executor gets full context
        tool_desc = tool_meta.description
        arg_hint = tool_meta.arg_hint or ""

        # Create full context message for Chat
        context_message = f"""ä»»åŠ¡ï¼š{task_context}

è¯·è°ƒç”¨ {tool_name} å·¥å…·æ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚
å·¥å…·æè¿°ï¼š{tool_desc}
å‚æ•°æç¤ºï¼š{arg_hint}

åªå…è®¸è°ƒç”¨ {tool_name} å·¥å…·å’Œ ask_user å·¥å…·ï¼ˆå¦‚æœéœ€è¦æ¾„æ¸…ä¿¡æ¯ï¼‰ã€‚
"""

    # Add context to messages
    execution_messages = state["messages"].copy()
    execution_messages.append(Message(role=Role.USER, content=context_message))

    # Create LLM client
    llm_client = create_llm_client(executor)

    retry_count = 0
    max_retries = 2

    while retry_count <= max_retries:
        try:
            # Step 1: Generate tool call
            # For Reasoner, provide only the specific tool function definition
            # For Chat, provide both the tool and ask_user functions
            if executor == "reasoner":
                functions = [{
                    "name": tool_name,
                    "description": tool_desc,  # Use truncated description for Reasoner
                    "parameters": tool_meta.parameters
                }, {
                    "name": "ask_user",
                    "description": "å‘ç”¨æˆ·è¯¢é—®ç¼ºå¤±çš„ä¿¡æ¯",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "question": {"type": "string", "description": "è¦é—®ç”¨æˆ·çš„é—®é¢˜"}
                        },
                        "required": ["question"]
                    }
                }]
            else:
                functions = [{
                    "name": tool_name,
                    "description": tool_meta.description,  # Full description for Chat
                    "parameters": tool_meta.parameters
                }, {
                    "name": "ask_user",
                    "description": "å‘ç”¨æˆ·è¯¢é—®ç¼ºå¤±çš„ä¿¡æ¯",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "question": {"type": "string", "description": "è¦é—®ç”¨æˆ·çš„é—®é¢˜"}
                        },
                        "required": ["question"]
                    }
                }]

            response = llm_client.generate(
                messages=execution_messages,
                functions=functions,
                stream=False
            )

            # Check if we got tool calls
            if not response.tool_calls:
                retry_count += 1
                if retry_count <= max_retries:
                    logger.warning(f"No tool calls generated, retrying ({retry_count}/{max_retries})")
                    continue
                else:
                    return {"success": False, "error": "No tool calls generated after retries"}

            # Process tool calls
            for tool_call in response.tool_calls:
                if tool_call.name == "ask_user":
                    # Need user clarification - this would trigger ask_user_interrupt
                    question = tool_call.arguments.get("question", "éœ€è¦æ›´å¤šä¿¡æ¯")
                    logger.info(f"Tool execution needs clarification: {question}")
                    return {"success": False, "needs_clarification": question}

                elif tool_call.name == tool_name:
                    # Execute the tool
                    try:
                        result = registry.execute(tool_name, **tool_call.arguments)
                        state["tool_call_count"] += 1

                        # Add tool call to conversation
                        assistant_msg = Message(
                            role=Role.ASSISTANT,
                            content="",
                            tool_calls=[ToolCall(
                                id=tool_call.id or f"call_{state['tool_call_count']}",
                                name=tool_call.name,
                                arguments=tool_call.arguments
                            )]
                        )
                        state["messages"].append(assistant_msg)

                        # Add tool response
                        tool_msg = Message(
                            role=Role.TOOL,
                            content=str(result.value) if hasattr(result, 'value') and result.value else str(result.error or "Tool executed"),
                            tool_call_id=tool_call.id or f"call_{state['tool_call_count']}"
                        )
                        state["messages"].append(tool_msg)

                        # Step 2: Generate summary
                        summary_messages = state["messages"].copy()
                        summary_messages.append(Message(
                            role=Role.USER,
                            content="è¯·åŸºäºå·¥å…·æ‰§è¡Œç»“æœï¼Œç»™å‡º1-2å¥ç®€æ´çš„æ€»ç»“ã€‚"
                        ))

                        summary_response = llm_client.generate(
                            messages=summary_messages,
                            stream=False
                        )

                        return {
                            "success": True,
                            "result": result,
                            "summary": summary_response.content,
                            "executor": executor
                        }

                    except Exception as e:
                        logger.error(f"Tool execution failed: {e}")
                        return {"success": False, "error": str(e)}

            # If we get here, no valid tool calls
            return {"success": False, "error": "No valid tool calls found"}

        except Exception as e:
            retry_count += 1
            if retry_count <= max_retries:
                logger.warning(f"Tool call failed, retrying ({retry_count}/{max_retries}): {e}")
            else:
                return {"success": False, "error": f"Tool call failed after retries: {e}"}

    return {"success": False, "error": "Unexpected error in tool execution"}


def _compress_conversation_history(messages: List[Message]) -> List[Message]:
    """Compress conversation history when it gets too long."""
    if len(messages) <= 2:
        return messages

    # Keep the first system message and last few exchanges
    compressed = []

    # Always keep system messages
    for msg in messages:
        if msg.role.value == "system":
            compressed.append(msg)

    # Keep last 3 user/assistant pairs
    recent_messages = []
    for msg in reversed(messages):
        if msg.role.value in ["user", "assistant"]:
            recent_messages.insert(0, msg)
            if len(recent_messages) >= 6:  # 3 pairs
                break

    compressed.extend(recent_messages)

    # Add a summary message
    summary_msg = Message(
        role=Role.SYSTEM,
        content="å†å²å¯¹è¯å·²å‹ç¼©ä»¥æé«˜æ€§èƒ½ã€‚ä¿ç•™äº†å…³é”®ç³»ç»Ÿä¿¡æ¯å’Œæœ€è¿‘çš„å¯¹è¯å†…å®¹ã€‚"
    )
    compressed.insert(0, summary_msg)

    logger.info(f"Compressed {len(messages)} messages to {len(compressed)}")
    return compressed


def get_next_executable_todos(state: AgentState) -> List[TodoItem]:
    """
    Determine which todos are ready for execution based on dependencies and parallel groups.
    Returns a list of todos that can be executed (may be multiple for parallel execution).
    """
    plan = state["plan"]
    completed_todos = {t.id for t in plan if t.output is not None}

    # Group todos by parallel groups
    parallel_groups = {}
    serial_todos = []

    for todo in plan:
        if todo.parallel_group:
            if todo.parallel_group not in parallel_groups:
                parallel_groups[todo.parallel_group] = []
            parallel_groups[todo.parallel_group].append(todo)
        else:
            serial_todos.append(todo)

    executable_todos = []

    # Check serial todos (those with dependencies)
    for todo in serial_todos:
        if todo.output is not None:
            continue  # Already completed

        # Check if all dependencies are completed
        deps_completed = all(dep_id in completed_todos for dep_id in (todo.dependencies or []))
        if deps_completed:
            executable_todos.append(todo)

    # Check parallel groups
    for group_id, group_todos in parallel_groups.items():
        # Group todos by execution order
        by_order = {}
        for todo in group_todos:
            order = todo.execution_order
            if order not in by_order:
                by_order[order] = []
            by_order[order].append(todo)

        # Execute todos in order, but allow parallel execution within the same order
        min_order = min(by_order.keys())
        current_order_todos = [t for t in by_order[min_order] if t.output is None]

        # Check if previous orders in this group are completed
        prev_orders_completed = True
        for order in range(1, min_order):
            if order in by_order:
                if not all(t.output is not None for t in by_order[order]):
                    prev_orders_completed = False
                    break

        if prev_orders_completed:
            executable_todos.extend(current_order_todos)

    return executable_todos


def todo_dispatch_node(state: AgentState) -> Dict[str, Any]:
    """
    Dispatch the next executable todo items for execution with proper serial/parallel control.

    Handles execution dependencies, parallel groups, and user interaction requirements.
    """
    logger.info("ğŸ”„ EXECUTION DISPATCH - Checking next executable todos...")

    if not state["plan"]:
        logger.warning("âŒ No plan available for dispatch")
        state["should_end"] = True
        state["current_node"] = "end"
        return state

    # Check if all todos are completed
    all_completed = all(todo.output is not None for todo in state["plan"])
    if all_completed:
        logger.info("ğŸ‰ EXECUTION COMPLETE - All todos finished successfully")
        state["should_end"] = True
        state["current_node"] = "end"
        return state

    # Get next executable todos (may be multiple for parallel execution)
    executable_todos = get_next_executable_todos(state)

    if not executable_todos:
        logger.warning("â³ EXECUTION WAITING - No todos ready (dependencies not satisfied)")
        state["should_end"] = True  # This will trigger ask_user if needed
        state["current_node"] = "end"
        return state

    # If multiple todos are executable, we need to decide execution strategy
    if len(executable_todos) > 1:
        logger.info(f"âš–ï¸ MULTIPLE EXECUTABLE - {len(executable_todos)} todos ready: {[t.id for t in executable_todos]}")

        # Check if they are in the same parallel group
        parallel_groups = set(t.parallel_group for t in executable_todos if t.parallel_group)
        if len(parallel_groups) == 1:
            logger.info("ğŸ”€ PARALLEL EXECUTION - Same group, executing concurrently")
            # For now, execute the first one and mark others as pending
            # TODO: Implement true parallel execution
            todo = executable_todos[0]
        else:
            # Different groups or mixed, execute first available
            logger.info("ğŸ”€ SEQUENTIAL EXECUTION - Different groups, executing first available")
            todo = executable_todos[0]
    else:
        todo = executable_todos[0]

    logger.info(f"ğŸš€ EXECUTING TODO - {todo.id}: {todo.title} (type: {todo.type.value})")

    # Check if this todo needs user input (new mechanism)
    if todo.needs and len(todo.needs) > 0:
        # Check if we already have user input for these needs
        has_all_needed = True
        missing_params = []

        for param in todo.needs:
            current_value = todo.input_data.get(param) if todo.input_data else None
            # Check if value is missing or is a placeholder
            if not current_value or current_value in ["éœ€è¦ç”¨æˆ·æŒ‡å®š", "ç”¨æˆ·å°†æä¾›", "å°†åœ¨æ‰§è¡Œæ—¶ä»ç”¨æˆ·è¾“å…¥è·å–"]:
                missing_params.append(param)
                has_all_needed = False

        if not has_all_needed:
            logger.info(f"ğŸ‘¤ USER INPUT REQUIRED - Todo {todo.id} missing: {missing_params}")
            logger.info(f"   Todo: {todo.title}")
            # Set state for user input collection and end execution to return to CLI
            state["needs_user_input"] = {
                "todo_id": todo.id,
                "needs": missing_params,
                "todo_title": todo.title
            }
            state["should_end"] = True
            state["current_node"] = "end"
            return state

    try:
        if todo.type == TodoType.TOOL:
            # Tool and executor should already be decided in planning phase
            if not todo.tool:
                logger.error(f"Tool todo {todo.id} missing tool name from planning phase")
                todo.output = f"é”™è¯¯ï¼šè§„åˆ’é˜¶æ®µæœªæŒ‡å®šå·¥å…·åç§°"
                state["current_todo"] = state.get("current_todo", 0) + 1
            else:
                # Executor should be set in planning phase, fallback to auto-resolution
                executor = todo.executor if todo.executor and todo.executor != "auto" else resolve_executor(todo)

                # Set up tool call for tool_exec node
                state["pending_tool_call"] = {
                    "tool": todo.tool,
                    "input_data": todo.input_data or {},
                    "todo_id": todo.id,
                    "executor": executor
                }
                logger.info(f"ğŸ”§ TOOL CALL PREPARED - {todo.tool} with executor {executor}")
                logger.info(f"   Input: {todo.input_data or {}}")
                logger.info(f"   Expected: {todo.expected_output or 'N/A'}")
                if todo.why:
                    logger.info(f"   Reason: {todo.why}")
                # Don't advance current_todo yet - wait for tool_exec completion

        elif todo.type == TodoType.CHAT:
            # Use chat model for this todo
            system_prompt = load_system_prompt("chat", "chat")
            additional_context = f"\n\nå½“å‰æ‰§è¡Œä»»åŠ¡ï¼š{todo.title}\næœŸæœ›è¾“å‡ºï¼š{todo.expected_output}"

            llm_client = create_llm_client("chat")
            response = llm_client.generate(
                messages=state["messages"],
                system_prompt=system_prompt + additional_context,
                stream=False
            )

            # Add response to messages
            assistant_msg = Message(role=Role.ASSISTANT, content=response.content)
            state["messages"].append(assistant_msg)

            # Store result in todo
            todo.output = response.content
            logger.info(f"Chat model executed for todo {todo.id}")

        elif todo.type in [TodoType.REASON, TodoType.WRITE, TodoType.RESEARCH]:
            # Use reasoner model for complex tasks
            system_prompt = load_system_prompt("reasoner", "reasoner")
            additional_context = f"\n\nå½“å‰æ‰§è¡Œä»»åŠ¡ï¼š{todo.title}\næœŸæœ›è¾“å‡ºï¼š{todo.expected_output}"

            llm_client = create_llm_client("reasoner")
            response = llm_client.generate(
                messages=state["messages"],
                system_prompt=system_prompt + additional_context,
                stream=False
            )

            # Add response to messages
            assistant_msg = Message(role=Role.ASSISTANT, content=response.content)
            state["messages"].append(assistant_msg)

            # Store result in todo
            todo.output = response.content
            logger.info(f"Reasoner model executed for todo {todo.id}")

        # Check if this todo needs sub-planning
        if todo.type in [TodoType.REASON, TodoType.RESEARCH] and state["depth_budget"] > 0:
            complexity_indicators = ['åˆ†è§£', 'ç»†åŒ–', 'å­ä»»åŠ¡', 'å¤šæ­¥éª¤', 'åˆ†é˜¶æ®µ']
            title_lower = todo.title.lower()
            output_lower = (todo.expected_output or "").lower()
            needs_subplanning = any(
                indicator in title_lower or indicator in output_lower
                for indicator in complexity_indicators
            )
            if needs_subplanning:
                logger.info(f"Todo {todo.id} may benefit from sub-planning (depth_budget: {state['depth_budget']})")
                # Could trigger planner_refine_local here

        # For tool todos, execution will continue in tool_exec node
        # For other types, we've completed execution here

    except Exception as e:
        logger.error(f"Error executing todo {todo.id}: {e}")
        # Mark todo as failed but continue
        todo.output = f"æ‰§è¡Œå¤±è´¥ï¼š{str(e)}"

    # Don't advance current_todo here - let the execution flow handle it
    # For tool calls, tool_exec node will handle completion
    # For direct execution, the todo is already marked as completed above

    state["execution_path"].append("todo_dispatch")
    state["current_node"] = "todo_dispatch"

    return state


def log_execution_metrics(state: AgentState) -> None:
    """
    Log execution metrics for observability.

    Args:
        state: Current agent state with metrics
    """
    metrics = state.get("metrics", {})

    if not metrics:
        logger.info("No execution metrics recorded")
        return

    # Log micro-reasoning metrics
    micro_calls = metrics.get("reasoner_micro_calls", 0)
    micro_empty = metrics.get("reasoner_micro_empty", 0)
    micro_fallback = metrics.get("reasoner_micro_fallback", 0)

    if micro_calls > 0:
        empty_rate = micro_empty / micro_calls * 100 if micro_calls > 0 else 0
        fallback_rate = micro_fallback / micro_calls * 100 if micro_calls > 0 else 0

        logger.info(f"Micro-reasoning metrics: calls={micro_calls}, empty={micro_empty} ({empty_rate:.1f}%), "
                   f"fallback={micro_fallback} ({fallback_rate:.1f}%)")

    # Log execution summary
    plan_size = len(state.get("plan", []))
    completed_todos = sum(1 for todo in state.get("plan", []) if todo.output)
    tool_calls = state.get("tool_call_count", 0)

    logger.info(f"Execution summary: plan_size={plan_size}, completed={completed_todos}, "
               f"tool_calls={tool_calls}, execution_path={state.get('execution_path', [])}")


def aggregate_answer_node(state: AgentState) -> Dict[str, Any]:
    """
    Aggregate results from all completed todos into final answer.
    """
    logger.info("ğŸ“Š AGGREGATING FINAL RESULTS - Compiling execution summary")

    # Log execution metrics
    log_execution_metrics(state)

    # Collect all assistant responses from the execution
    assistant_responses = [
        msg.content for msg in state["messages"]
        if msg.role.value == "assistant"
    ]

    logger.info(f"ğŸ“‹ EXECUTION SUMMARY - {len(state['plan']) if state['plan'] else 0} todos completed")

    # Log detailed results for each todo
    if state["plan"]:
        for i, todo in enumerate(state["plan"], 1):
            status = "âœ…" if todo.output else "âŒ"
            logger.info(f"   {i}. {status} {todo.id}: {todo.title}")
            if todo.output:
                output_preview = todo.output[:100] + "..." if len(todo.output) > 100 else todo.output
                logger.info(f"      Result: {output_preview}")
            else:
                logger.info("      Result: æœªæ‰§è¡Œ")

    # Create a comprehensive answer
    if state["plan"]:
        final_answer = f"## æ‰§è¡Œå®Œæˆ\n\n"
        for i, todo in enumerate(state["plan"]):
            final_answer += f"### {todo.title}\n{todo.why}\n\n"

        final_answer += f"## ç»“æœæ±‡æ€»\n\n"
        for i, response in enumerate(assistant_responses[-len(state["plan"]):]):
            final_answer += f"**æ­¥éª¤{i+1}ç»“æœï¼š**\n{response}\n\n"
    else:
        final_answer = "ä»»åŠ¡æ‰§è¡Œå®Œæˆã€‚" + " ".join(assistant_responses[-1:])

    logger.info("ğŸ¯ FINAL ANSWER GENERATED")
    logger.info(f"   Length: {len(final_answer)} characters")
    logger.info(f"   Preview: {final_answer[:200]}...")

    state["final_answer"] = final_answer
    state["should_end"] = True

    state["execution_path"].append("aggregate_answer")
    state["current_node"] = "aggregate_answer"

    return state


# ===== REFLECTIVE REPLANNING NODES =====

def reflective_replanning_check_node(state: AgentState) -> Dict[str, Any]:
    """
    Check if reflective replanning is needed after major information gathering.

    This node is called after tool execution to determine if we should trigger
    the expensive reflective replanning process based on new information volume.
    """
    from backend.config import Config

    # Skip if reflective replanning is disabled
    if not Config.ENABLE_REFLECTIVE_REPLANNING:
        logger.debug("Reflective replanning disabled, skipping")
        state["execution_path"].append("reflective_check")
        state["current_node"] = "reflective_check"
        return state

    # Check if we just executed a tool that might have gathered significant information
    if not state.get("pending_tool_call") or not state.get("last_tool_result"):
        logger.debug("No recent tool execution to check for reflection")
        state["execution_path"].append("reflective_check")
        state["current_node"] = "reflective_check"
        return state

    tool_name = state["pending_tool_call"]["tool"]
    tool_result = state["last_tool_result"]

    # Check if this tool typically gathers large amounts of information
    info_gathering_tools = ["file_read", "web_search", "rag_search", "tabular_qa"]

    if tool_name not in info_gathering_tools:
        logger.debug(f"Tool {tool_name} not considered info-gathering, skipping reflection")
        state["execution_path"].append("reflective_check")
        state["current_node"] = "reflective_check"
        return state

    # Estimate the amount of new information gathered
    info_size = 0

    # Check tool result for content
    if "value" in tool_result and isinstance(tool_result["value"], dict):
        tool_value = tool_result["value"]
        if "content" in tool_value:
            info_size += len(str(tool_value["content"]))
        if "summary" in tool_result:
            info_size += len(str(tool_result["summary"]))

    # Check if information size exceeds threshold
    if info_size >= Config.REFLECTIVE_REPLANNING_MIN_INFO_SIZE:
        logger.info(f"ğŸ§  DETECTED LARGE INFO GATHERING - {tool_name} returned ~{info_size} chars, triggering reflective replanning")
        state["trigger_reflective_replanning"] = True
        state["new_information_summary"] = _extract_information_summary(tool_result, tool_name)
    else:
        logger.debug(f"Information size {info_size} below threshold {Config.REFLECTIVE_REPLANNING_MIN_INFO_SIZE}, skipping reflection")

    state["execution_path"].append("reflective_check")
    state["current_node"] = "reflective_check"

    return state


def reflective_replanning_node(state: AgentState) -> Dict[str, Any]:
    """
    Perform reflective replanning after major information gathering.

    This expensive process compresses context and allows the AI to reconsider
    and potentially modify the execution plan based on new information.
    """
    from backend.config import Config

    if not state.get("trigger_reflective_replanning"):
        logger.debug("No reflective replanning triggered")
        state["execution_path"].append("reflective_replanning")
        state["current_node"] = "reflective_replanning"
        return state

    logger.info("ğŸ§  STARTING REFLECTIVE REPLANNING - Compressing context and evaluating plan changes")

    try:
        # Phase 1: Chat model compresses information and current plan
        chat_client = create_llm_client("chat")

        current_plan = state.get("plan", [])
        new_info = state.get("new_information_summary", "")

        compression_prompt = f"""åŸºäºæ–°è·å–çš„ä¿¡æ¯ï¼Œå‹ç¼©å½“å‰è®¡åˆ’çš„å…³é”®ä¿¡æ¯åˆ°{Config.REFLECTIVE_REPLANNING_MAX_TOKENS}tokenå†…ï¼š

æ–°è·å–çš„ä¿¡æ¯ï¼š
{new_info[:2000]}...ï¼ˆå¦‚æœ‰æ›´å¤šå†…å®¹å·²çœç•¥ï¼‰

å½“å‰æ‰§è¡Œè®¡åˆ’ï¼š
{_summarize_current_plan(current_plan)}

è¯·ç”¨<100å­—æ€»ç»“ï¼š
1. æ–°ä¿¡æ¯çš„æ ¸å¿ƒè¦ç‚¹
2. å½“å‰è®¡åˆ’çš„çŠ¶æ€
3. æ½œåœ¨çš„è®¡åˆ’è°ƒæ•´æ–¹å‘

è¾“å‡ºæ ¼å¼ï¼šç›´æ¥ç»™å‡ºæ€»ç»“ï¼Œä¸è¦å¤šä½™å†…å®¹ã€‚"""

        logger.info("ğŸ§  REFLECTIVE PHASE 1 - Chat compressing context")
        compression_response = chat_client.generate(
            messages=[Message(role=Role.USER, content=compression_prompt)],
            stream=False
        )
        compressed_context = compression_response.content.strip()
        logger.info(f"ğŸ§  COMPRESSED CONTEXT ({len(compressed_context)} chars): {compressed_context[:200]}...")

        # Phase 2: Reasoner evaluates if plan changes are needed
        # Get available tools for context
        try:
            from backend.tool_prompt_builder import build_tool_prompts
            tool_prompts = build_tool_prompts()
            available_tools = tool_prompts.get("tool_name_index", {})
            tools_list = list(available_tools.keys())
        except Exception as e:
            logger.warning(f"Could not load tools for reflection: {e}")
            tools_list = ["file_read", "web_search", "rag_search", "tabular_qa", "calculator", "datetime", "markdown_writer"]

        reasoner_client = create_llm_client("reasoner")

        reflection_prompt = f"""è¯„ä¼°æ˜¯å¦éœ€è¦ä¿®æ”¹æ‰§è¡Œè®¡åˆ’ï¼š

å‹ç¼©çš„ä¸Šä¸‹æ–‡ï¼š{compressed_context}

å¯ç”¨å·¥å…·åˆ—è¡¨ï¼š{', '.join(tools_list)}

é‡è¦è¯´æ˜ï¼šä½ åªèƒ½å»ºè®®ä½¿ç”¨ä¸Šè¿°åˆ—è¡¨ä¸­çš„å·¥å…·ï¼Œä¸èƒ½å‘æ˜æˆ–å‡è®¾ä¸å­˜åœ¨çš„å·¥å…·ï¼

è¯·åˆ¤æ–­ï¼š
- å½“å‰è®¡åˆ’æ˜¯å¦ä»åˆé€‚ï¼Ÿ
- æ–°ä¿¡æ¯æ˜¯å¦éœ€è¦è°ƒæ•´æ­¥éª¤ï¼Ÿ
- æ˜¯å¦éœ€è¦æ·»åŠ /åˆ é™¤/ä¿®æ”¹ä»»åŠ¡ï¼Ÿ

å¦‚æœéœ€è¦ä¿®æ”¹ï¼Œç»™å‡ºå…·ä½“çš„ä¿®æ”¹å»ºè®®ï¼ˆå¿…é¡»ä½¿ç”¨ä¸Šè¿°å¯ç”¨å·¥å…·ï¼‰ã€‚
å¦‚æœä¸éœ€è¦ä¿®æ”¹ï¼Œåªå›å¤"æ— éœ€ä¿®æ”¹"ã€‚

è¾“å‡ºæ ¼å¼ï¼šç®€æ´çš„åˆ¤æ–­å’Œå»ºè®®ã€‚"""

        logger.info("ğŸ§  REFLECTIVE PHASE 2 - Reasoner evaluating plan changes")
        reflection_response = reasoner_client.generate(
            messages=[Message(role=Role.USER, content=reflection_prompt)],
            stream=False
        )
        reflection_result = reflection_response.content.strip()
        logger.info(f"ğŸ§  REFLECTION RESULT: {reflection_result[:200]}...")

        # Check if changes are needed
        if "æ— éœ€ä¿®æ”¹" not in reflection_result.lower() and len(reflection_result.strip()) > 0:
            # Phase 3: Chat model modifies the plan based on reflection
            logger.info("ğŸ§  REFLECTIVE PHASE 3 - Chat modifying plan based on reflection")

            plan_modification_prompt = f"""åŸºäºåæ€ç»“æœä¿®æ”¹æ‰§è¡Œè®¡åˆ’ï¼š

å‹ç¼©ä¸Šä¸‹æ–‡ï¼š{compressed_context}

åæ€å»ºè®®ï¼š{reflection_result}

å½“å‰è®¡åˆ’è¯¦æƒ…ï¼š
{_format_plan_for_modification(current_plan)}

è¯·è¾“å‡ºä¿®æ”¹åçš„å®Œæ•´JSONæ ¼å¼æ‰§è¡Œè®¡åˆ’ã€‚æ ¼å¼è¦æ±‚ä¸åŸå§‹è®¡åˆ’ç›¸åŒã€‚

æ³¨æ„ï¼šåªèƒ½ä½¿ç”¨ç³»ç»Ÿå·²æœ‰çš„å·¥å…·ï¼Œä¸è¦å‘æ˜ä¸å­˜åœ¨çš„å·¥å…·ã€‚"""

            modification_response = chat_client.generate(
                messages=[Message(role=Role.USER, content=plan_modification_prompt)],
                stream=False
            )

            try:
                import json
                import re
                content = modification_response.content.strip()

                # Try to extract JSON from response
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    modified_plan_data = json.loads(json_match.group())
                    logger.info("ğŸ§  PLAN MODIFIED - Applying new execution plan")

                    # Update the plan in state
                    state["plan"] = _create_todos_from_json(modified_plan_data)
                    state["plan_modified"] = True
                else:
                    logger.warning("ğŸ§  PLAN MODIFICATION FAILED - Could not parse JSON")
                    state["plan_modified"] = False

            except Exception as e:
                logger.error(f"ğŸ§  PLAN MODIFICATION ERROR: {e}")
                state["plan_modified"] = False
        else:
            logger.info("ğŸ§  NO PLAN CHANGES NEEDED - Continuing with current plan")
            state["plan_modified"] = False

    except Exception as e:
        logger.error(f"ğŸ§  REFLECTIVE REPLANNING ERROR: {e}")
        state["plan_modified"] = False

    # Clear the trigger
    state.pop("trigger_reflective_replanning", None)
    state.pop("new_information_summary", None)

    state["execution_path"].append("reflective_replanning")
    state["current_node"] = "reflective_replanning"

    return state


def _extract_information_summary(tool_result: dict, tool_name: str) -> str:
    """Extract and summarize information from tool result."""
    summary_parts = []

    if "value" in tool_result and isinstance(tool_result["value"], dict):
        tool_value = tool_result["value"]

        if tool_name == "file_read" and "content" in tool_value:
            content = tool_value["content"]
            summary_parts.append(f"æ–‡ä»¶å†…å®¹ï¼š{content[:500]}..." if len(content) > 500 else f"æ–‡ä»¶å†…å®¹ï¼š{content}")

        elif tool_name == "web_search" and "results" in tool_value:
            results = tool_value["results"]
            summary_parts.append(f"ç½‘ç»œæœç´¢ç»“æœï¼šæ‰¾åˆ°{len(results)}ä¸ªç»“æœ")

        elif tool_name == "rag_search" and "results" in tool_value:
            results = tool_value["results"]
            summary_parts.append(f"RAGæœç´¢ç»“æœï¼šæ‰¾åˆ°{len(results)}ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")

        elif tool_name == "tabular_qa" and "answer" in tool_value:
            summary_parts.append(f"è¡¨æ ¼é—®ç­”ç»“æœï¼š{tool_value['answer']}")

    if "summary" in tool_result:
        summary_parts.append(f"å·¥å…·æ€»ç»“ï¼š{tool_result['summary']}")

    return " | ".join(summary_parts) if summary_parts else "æ— æ–°ä¿¡æ¯"


def _summarize_current_plan(plan: List) -> str:
    """Summarize the current execution plan."""
    if not plan:
        return "æ— å½“å‰è®¡åˆ’"

    summaries = []
    for i, todo in enumerate(plan, 1):
        status = "âœ…" if todo.output else "â³" if todo.input_data else "âŒ"
        summaries.append(f"{i}. {status} {todo.title}")

    return "\n".join(summaries)


def _format_plan_for_modification(plan: List) -> str:
    """Format plan for modification prompt."""
    if not plan:
        return "å½“å‰è®¡åˆ’ä¸ºç©º"

    formatted = []
    for todo in plan:
        status = "å·²å®Œæˆ" if todo.output else "å¾…æ‰§è¡Œ"
        formatted.append(f"""
- ID: {todo.id}
  æ ‡é¢˜: {todo.title}
  çŠ¶æ€: {status}
  ç±»å‹: {todo.type.value}
  å·¥å…·: {todo.tool or 'N/A'}
  ä¾èµ–: {todo.dependencies or []}
  å¹¶è¡Œç»„: {todo.parallel_group or 'N/A'}
  æ‰§è¡Œé¡ºåº: {todo.execution_order}
  éœ€è¦å‚æ•°: {todo.needs or []}
  é¢„æœŸè¾“å‡º: {todo.expected_output}
""")

    return "".join(formatted)


def _create_todos_from_json(plan_data: dict) -> List:
    """Create Todo objects from JSON plan data."""
    from backend.message_types import Todo, TodoType

    todos = []
    if "todos" in plan_data:
        for todo_data in plan_data["todos"]:
            try:
                todo = Todo(
                    id=todo_data["id"],
                    title=todo_data["title"],
                    why=todo_data.get("why", ""),
                    type=TodoType(todo_data.get("type", "tool")),
                    tool=todo_data.get("tool"),
                    executor=todo_data.get("executor", "chat"),
                    dependencies=todo_data.get("dependencies", []),
                    parallel_group=todo_data.get("parallel_group"),
                    execution_order=todo_data.get("execution_order", 0),
                    input=todo_data.get("input", {}),
                    expected_output=todo_data.get("expected_output", ""),
                    needs=todo_data.get("needs", [])
                )
                todos.append(todo)
            except Exception as e:
                logger.error(f"Error creating todo from {todo_data}: {e}")

    return todos


