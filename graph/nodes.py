"""
Node definitions for the DigitalClone AI Assistant LangGraph.

This module contains all the node functions that make up the execution graph.
"""

import logging
import json
import re
from typing import Dict, Any, List, Optional

import sys
import os
from pathlib import Path

# Add the project root to Python path for proper imports
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from langchain_core.messages import HumanMessage, AIMessage, ToolMessage

# Import state types first (always needed)
from .state import AgentState, Route, TodoItem, TodoType

# Conditional imports to support both relative and absolute imports
try:
    from backend.message_types import Message, Role, RouteDecision, ToolCall
    from backend.agent_core import AgentRouter
    from backend.llm_interface import create_llm_client
    from backend.tool_registry import registry
    from backend.tool_prompt_builder import build_tool_prompts, load_system_prompt
    from backend.config import config
    from backend.logger import ConversationLogger
    from backend.ask_user_policy import needs_user_clarification
except ImportError:
    # Fallback to relative imports if absolute imports fail
    from message_types import Message, Role, RouteDecision, ToolCall
    from agent_core import AgentRouter
    from llm_interface import create_llm_client
    from tool_registry import registry
    from tool_prompt_builder import build_tool_prompts, load_system_prompt
    from config import config
    from logger import ConversationLogger

logger = logging.getLogger(__name__)


def user_input_node(state: AgentState) -> Dict[str, Any]:
    """
    User input processing node.

    This node receives user input and prepares the initial state.
    In our implementation, this is handled by create_initial_state,
    so this node mainly serves as an entry point.
    """
    logger.info(f"Processing user input: {state['messages'][-1].content[:50]}...")

    # Update execution path
    state["execution_path"].append("user_input")
    state["current_node"] = "user_input"

    # Return unchanged state - routing will happen next
    return state


def decide_route_node(state: AgentState) -> Dict[str, Any]:
    """
    Route decision node.

    Decides whether to use chat or reasoner model based on the input.
    """
    logger.info("Making routing decision...")

    if not state["messages"]:
        raise ValueError("No messages in state")

    user_message = None
    for msg in reversed(state["messages"]):
        if msg.role == Role.USER:
            user_message = msg
            break

    if not user_message:
        raise ValueError("No user message found")

    # Use existing router logic
    router = AgentRouter()
    route_decision = router.route(user_message.content, None)  # Simplified context

    # Map to our Route enum
    if route_decision.engine == "reasoner":
        route = Route.REASONER
    else:
        route = Route.CHAT

    logger.info(f"Routing decision: {route.value} (reason: {route_decision.reason})")

    # Update state
    state["route"] = route
    state["route_decision"] = route_decision
    state["execution_path"].append("decide_route")
    state["current_node"] = "decide_route"

    return state


def model_call_node(state: AgentState) -> Dict[str, Any]:
    """
    Model call node.

    Calls the appropriate LLM (chat or reasoner) with tools and system prompts.
    """
    logger.info(f"Making model call with route: {state['route'].value}")

    route = state["route"]
    messages = state["messages"]

    # Load appropriate system prompt
    try:
        from backend.tool_prompt_builder import load_system_prompt
    except ImportError:
        from tool_prompt_builder import load_system_prompt
    system_prompt = load_system_prompt(route.value)

    # Select appropriate LLM client
    if route == Route.REASONER:
        llm_client = create_llm_client("reasoner")
    else:
        llm_client = create_llm_client("chat")

    # Build tool prompts
    tool_prompts = build_tool_prompts()
    functions = tool_prompts["tools"]

    # For now, we'll handle streaming separately in CLI
    # This node focuses on the core logic
    response = llm_client.generate(
        messages,
        functions=functions,
        system_prompt=system_prompt,
        stream=False
    )

    # Handle response
    if response.tool_calls:
        # Has tool calls - store for execution
        tool_call = response.tool_calls[0]
        state["pending_tool_call"] = {
            "name": tool_call.name,
            "arguments": tool_call.arguments,
            "id": getattr(tool_call, 'id', None)  # May not have ID yet
        }
        logger.info(f"Tool call detected: {tool_call.name}")

        # Add assistant message to conversation
        assistant_message = Message(
            role=Role.ASSISTANT,
            content=response.content,
            tool_call=tool_call
        )
        state["messages"].append(assistant_message)

    elif needs_user_clarification(response.content):
        # Needs user clarification
        state["awaiting_user"] = True
        state["user_input_buffer"] = None

        # Add assistant message
        assistant_message = Message(
            role=Role.ASSISTANT,
            content=response.content
        )
        state["messages"].append(assistant_message)

        logger.info("AskUser clarification needed")

    else:
        # Final answer
        state["final_answer"] = response.content
        state["should_end"] = True

        # Add assistant message
        assistant_message = Message(
            role=Role.ASSISTANT,
            content=response.content
        )
        state["messages"].append(assistant_message)

        logger.info("Final answer generated")

    state["execution_path"].append("model_call")
    state["current_node"] = "model_call"

    return state


def tool_exec_node(state: AgentState) -> Dict[str, Any]:
    """
    Tool execution node with executor routing and two-step protocol.

    Uses the specified executor to call tools with proper LLM interaction.
    """
    logger.info("Executing tool with executor routing...")

    if not state["pending_tool_call"]:
        raise ValueError("No pending tool call in state")

    tool_call_info = state["pending_tool_call"]
    tool_name = tool_call_info["tool"]
    input_data = tool_call_info["input_data"]
    todo_id = tool_call_info["todo_id"]
    executor = tool_call_info["executor"]

    logger.info(f"Executing tool: {tool_name} with executor: {executor}")

    # Find the corresponding todo item
    current_todo = None
    if state["plan"]:
        for todo in state["plan"]:
            if todo.id == todo_id:
                current_todo = todo
                break

    if not current_todo:
        logger.error(f"Could not find todo with id {todo_id}")
        state["pending_tool_call"] = None
        return state

    # Execute tool using the two-step protocol
    task_context = f"{current_todo.title}\nç›®æ ‡ï¼š{current_todo.expected_output}"
    tool_result = call_tool_with_llm(executor, tool_name, task_context, state)

    if tool_result["success"]:
        # Update todo with result
        current_todo.output = tool_result["summary"]
        logger.info(f"Tool {tool_name} executed successfully with {executor} executor")

        # Advance to next todo
        current_idx = state.get("current_todo", 0)
        state["current_todo"] = current_idx + 1

    else:
        if "needs_clarification" in tool_result:
            # Trigger ask_user_interrupt - pause the tool call for later resumption
            state["paused_tool_call"] = state["pending_tool_call"]  # Save for resumption
            state["pending_tool_call"] = None  # Clear current pending call
            state["awaiting_user"] = True
            state["user_input_buffer"] = tool_result["needs_clarification"]
            logger.info(f"Tool execution paused for user clarification: {tool_result['needs_clarification']}")
            return state
        else:
            # Mark as failed and continue
            current_todo.output = f"å·¥å…·æ‰§è¡Œå¤±è´¥ï¼š{tool_result['error']}"
            logger.error(f"Tool execution failed: {tool_result['error']}")

            # Advance to next todo despite failure
            current_idx = state.get("current_todo", 0)
            state["current_todo"] = current_idx + 1

    # Clear pending tool call
    state["pending_tool_call"] = None

    state["execution_path"].append("tool_exec")
    state["current_node"] = "tool_exec"

    return state


def need_user_node(state: AgentState) -> Dict[str, Any]:
    """
    Check if user clarification is needed.

    This is a conditional node that determines the flow.
    """
    needs_user = state["awaiting_user"]
    logger.info(f"User clarification needed: {needs_user}")

    state["execution_path"].append("need_user")
    state["current_node"] = "need_user"

    return state


def ask_user_interrupt_node(state: AgentState) -> Dict[str, Any]:
    """
    Human-in-the-loop interrupt node.

    This node handles resumption after user clarification.
    If there was a paused tool call, it will resume it.
    """
    logger.info("Resuming after user clarification")

    # Check if there's a paused tool call to resume
    if state.get("paused_tool_call"):
        # Resume the paused tool call
        state["pending_tool_call"] = state["paused_tool_call"]
        state["paused_tool_call"] = None
        logger.info("Resuming paused tool call")

    # Clear clarification state
    state["awaiting_user"] = False
    state["user_input_buffer"] = None

    state["execution_path"].append("ask_user_interrupt")
    state["current_node"] = "ask_user_interrupt"

    return state


def end_node(state: AgentState) -> Dict[str, Any]:
    """
    End node for successful completion.
    """
    logger.info("Conversation completed successfully")

    state["execution_path"].append("end")
    state["current_node"] = "end"
    state["should_end"] = True

    return state


# ===== PLANNER NODES =====

def classify_intent_node(state: AgentState) -> Dict[str, Any]:
    """
    Classify user intent and choose execution pipeline.

    Routes to: chat, planner, auto_rag
    Locks planner route until completion.
    """
    logger.info("Classifying user intent...")

    # Check if route is already locked (from previous planner execution)
    if state.get("route_locked", False):
        logger.info("Route locked, continuing with planner")
        state["execution_path"].append("classify_intent")
        state["current_node"] = "classify_intent"
        return state

    if not state["messages"]:
        raise ValueError("No messages in state")

    user_message = None
    for msg in reversed(state["messages"]):
        if msg.role.value == "user":
            user_message = msg
            break

    if not user_message:
        raise ValueError("No user message found")

    user_input = user_message.content.lower()

    # Check for explicit auto_rag triggers
    if any(keyword in user_input for keyword in ['è‡ªåŠ¨æ‰©å……', 'æ•´ç†å¯¹è¯', 'æ€»ç»“å¯¹è¯', 'auto_rag']):
        route = Route.AUTO_RAG
        reason = "ç”¨æˆ·æ˜ç¡®è¯·æ±‚è‡ªåŠ¨çŸ¥è¯†æ‰©å……"
    # Check for planning keywords (must match AgentRouter.COMPLEX_KEYWORDS)
    elif any(keyword in user_input for keyword in [
        'è®¡åˆ’', 'è§„åˆ’', 'åˆ¶å®š', 'åˆ†è§£', 'å¤šæ­¥éª¤', 'è°ƒç ”', 'å†™æ–¹æ¡ˆ', 'è¯„ä¼°', 'å¯¹æ¯”',
        'æµç¨‹', 'ä¾èµ–', 'é‡Œç¨‹ç¢‘', 'roadmap', 'strategy', 'systematic',
        'complex', 'comprehensive', 'detailed', 'step-by-step', 'breakdown',
        'åˆ†æ', 'æ€»ç»“', 'æŠ¥å‘Š', 'æŸ¥æ‰¾', 'æœç´¢', 'ç ”ç©¶', 'è°ƒæŸ¥', 'æ•´ç†',
        'ç»¼åˆ', 'æ•´åˆ', 'æ¯”è¾ƒ', 'è¯„ä¼°', 'æ’°å†™', 'ç”Ÿæˆ', 'åˆ›å»º'
    ]) or len(user_input) > 100:  # Long inputs likely need planning
        route = Route.PLANNER
        reason = "æ£€æµ‹åˆ°å¤æ‚è§„åˆ’ä»»åŠ¡ç‰¹å¾"
        # Lock the route for planner execution
        state["route_locked"] = True
    else:
        route = Route.CHAT
        reason = "ç®€å•å¯¹è¯ä»»åŠ¡ï¼Œä½¿ç”¨chatæ¨¡å‹"

    logger.info(f"Intent classification: {route.value} ({reason})")

    state["route"] = route
    state["execution_path"].append("classify_intent")
    state["current_node"] = "classify_intent"

    return state


def sufficiency_check_node(state: AgentState) -> Dict[str, Any]:
    """
    Check if we have sufficient information to proceed with planning.

    This is a simplified version - in production, you'd use more sophisticated analysis.
    """
    logger.info("Checking information sufficiency...")

    # For now, assume we have enough information
    # In production, this would analyze the plan and check for missing prerequisites
    state["sufficiency"] = "enough"
    state["execution_path"].append("sufficiency_check")
    state["current_node"] = "sufficiency_check"

    return state


def planner_generate_node(state: AgentState) -> Dict[str, Any]:
    """
    Generate a structured plan using the reasoner model with JSON mode.

    Uses response_format=json_object to ensure strict JSON output.
    """
    logger.info("Generating structured plan...")

    if not state["messages"]:
        raise ValueError("No messages in state")

    # Get the user request
    user_request = ""
    for msg in state["messages"]:
        if msg.role.value == "user":
            user_request = msg.content
            break

    # Load planner system prompt
    try:
        with open(project_root / "prompts" / "reasoner_planner.txt", 'r', encoding='utf-8') as f:
            system_prompt = f.read()
    except FileNotFoundError:
        # Fallback system prompt
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é¡¹ç›®è§„åˆ’å¸ˆã€‚ç”¨æˆ·ä¼šæå‡ºä¸€ä¸ªå¤æ‚ä»»åŠ¡ï¼Œä½ éœ€è¦å°†å…¶åˆ†è§£ä¸ºå…·ä½“çš„ã€å¯æ‰§è¡Œçš„æ­¥éª¤ã€‚

åªè¾“å‡ºJSONæ ¼å¼çš„è®¡åˆ’ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚æ ¼å¼å¦‚ä¸‹ï¼š
{
  "goal": "ä»»åŠ¡ç›®æ ‡æè¿°",
  "success_criteria": "æˆåŠŸæ ‡å‡†",
  "todos": [
    {
      "id": "T1",
      "title": "å…·ä½“æ­¥éª¤æ ‡é¢˜",
      "why": "ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸€æ­¥",
      "type": "tool|chat|reason|write|research",
      "tool": "å·¥å…·åç§°ï¼ˆå¦‚æœtype=toolï¼‰",
      "input": {"å‚æ•°å": "å‚æ•°å€¼"},
      "expected_output": "æœŸæœ›çš„è¾“å‡ºæ ¼å¼",
      "needs": ["éœ€è¦çš„ç”¨æˆ·ä¿¡æ¯"]
    }
  ]
}"""

    # Create LLM client
    llm_client = create_llm_client("reasoner")

    # è·å–å®Œæ•´çš„å·¥å…·ä¿¡æ¯å’Œexecutoré€‰é¡¹
    tools_info = []
    for tool_name in registry.list_tools():
        tool_meta = registry.get_tool_meta(tool_name)
        if tool_meta:
            # ç¡®å®šé»˜è®¤executor
            default_executor = "chat"
            if tool_meta.executor_default == "reasoner" or tool_meta.complexity == "complex":
                default_executor = "reasoner"

            # åˆ›å»ºçº¯å­—å…¸ï¼Œé¿å…ToolMetaå¯¹è±¡çš„åºåˆ—åŒ–é—®é¢˜
            tools_info.append({
                "name": tool_name,
                "description": tool_meta.description[:100],  # é™åˆ¶é•¿åº¦
                "default_executor": default_executor,
                "complexity": tool_meta.complexity
            })

    # æ„å»ºè¯¦ç»†çš„å·¥å…·ä¿¡æ¯æ–‡æœ¬
    tools_text = "å¯ç”¨å·¥å…·å’Œæ‰§è¡Œå™¨ï¼š\n"
    for tool in tools_info:
        tools_text += f"- {tool['name']}ï¼ˆ{tool['description']}ï¼‰é»˜è®¤æ‰§è¡Œå™¨ï¼š{tool['default_executor']}ï¼Œå¤æ‚åº¦ï¼š{tool['complexity']}\n"
    tools_text += "\næ‰§è¡Œå™¨è¯´æ˜ï¼š\n- chatï¼šé€‚åˆç®€å•ä»»åŠ¡ï¼Œå“åº”å¿«\n- reasonerï¼šé€‚åˆå¤æ‚æ¨ç†ï¼Œå“åº”æ…¢ä½†å‡†ç¡®"

    # ä½¿ç”¨ä»æµ‹è¯•ä¸­éªŒè¯æˆåŠŸçš„promptæ ¼å¼
    user_prompt = f"""ç”¨æˆ·ä»»åŠ¡ï¼š{user_request}

è¯·åˆ¶å®šæ‰§è¡Œè®¡åˆ’ã€‚åªè¾“å‡ºJSONæ ¼å¼ï¼š

{{
  "goal": "ä»»åŠ¡ç›®æ ‡æè¿°",
  "success_criteria": "æˆåŠŸæ ‡å‡†",
  "todos": [
    {{
      "id": "T1",
      "title": "å…·ä½“æ­¥éª¤",
      "why": "ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸€æ­¥",
      "type": "tool",
      "tool": "calculator",
      "input": {{"expression": "2+2"}},
      "expected_output": "è®¡ç®—ç»“æœ",
      "needs": []
    }}
  ]
}}

{tools_text}"""

    try:
        # Use Chat model directly for planning (Reasoner has compatibility issues)
        logger.info("ğŸ¯ Using Chat model for planning (Reasoner has empty response issues)")
        chat_client = create_llm_client("chat")

        # Simplified planning prompt that works reliably with executor selection
        planning_prompt = f"""ä½ æ˜¯ä¸€ä¸ªé¡¹ç›®è§„åˆ’å¸ˆã€‚è¯·åˆ¶å®šæ‰§è¡Œè®¡åˆ’ï¼Œå¹¶ä¸ºæ¯ä¸ªå·¥å…·ä»»åŠ¡é€‰æ‹©åˆé€‚çš„æ‰§è¡Œå™¨ã€‚

ç”¨æˆ·éœ€æ±‚ï¼š{user_request}

{tools_text}

é‡è¦ï¼šå¯¹äºæ¯ä¸ªå·¥å…·ä»»åŠ¡ï¼Œä½ å¿…é¡»é€‰æ‹©"executor"å­—æ®µï¼Œå¯é€‰å€¼ï¼š"chat"æˆ–"reasoner"ã€‚
- chatï¼šå¿«é€Ÿå“åº”ï¼Œé€‚åˆç®€å•ä»»åŠ¡
- reasonerï¼šæ·±åº¦æ¨ç†ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡ï¼Œä½†å“åº”è¾ƒæ…¢

è¯·è¾“å‡ºJSONæ ¼å¼çš„è®¡åˆ’ï¼š

{{
  "goal": "ä»»åŠ¡ç›®æ ‡æè¿°",
  "success_criteria": "æˆåŠŸæ ‡å‡†",
  "todos": [
    {{
      "id": "T1",
      "title": "æ­¥éª¤æ ‡é¢˜",
      "why": "ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸€æ­¥",
      "type": "tool",
      "tool": "å·¥å…·åç§°",
      "executor": "chat",
      "input": {{"å‚æ•°": "å€¼"}},
      "expected_output": "é¢„æœŸè¾“å‡º",
      "needs": []
    }}
  ]
}}

åªè¾“å‡ºJSONã€‚"""

        response = chat_client.generate(
            messages=[Message(role=Role.USER, content=planning_prompt)],
            system_prompt=system_prompt,
            stream=False,
            response_format={"type": "json_object"}  # Chat model supports this reliably
        )

        # Parse the JSON response (Chat model should return valid JSON with response_format)
        content = response.content.strip()
        logger.debug(f"Raw planner response: {content[:500]}...")

        try:
            plan_data = json.loads(content)
            logger.info("âœ… Chat model JSON parsing successful")
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Chat model JSON parsing failed: {e}")
            logger.error(f"Response content: {content}")

            # Try to extract and fix JSON as fallback
            import re
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                try:
                    extracted_json = json_match.group()
                    plan_data = json.loads(extracted_json)
                    logger.info("âœ… JSON extracted and parsed from response")
                except json.JSONDecodeError:
                    raise ValueError(f"Failed to parse JSON from Chat model response: {content[:200]}...")
            else:
                raise ValueError(f"No JSON found in Chat model response: {content[:200]}...")

        # Validate JSON against schema (basic validation)
        required_keys = ["goal", "success_criteria", "todos"]
        for key in required_keys:
            if key not in plan_data:
                raise ValueError(f"Missing required key: {key}")

        if not isinstance(plan_data["todos"], list):
            raise ValueError("todos must be a list")

        # Convert to TodoItem objects
        todos = []
        for todo_data in plan_data["todos"]:
            # Validate todo structure
            required_todo_keys = ["id", "title", "type"]
            for key in required_todo_keys:
                if key not in todo_data:
                    raise ValueError(f"Todo missing required key: {key}")

            todos.append(TodoItem(
                id=todo_data["id"],
                title=todo_data["title"],
                why=todo_data.get("why", ""),
                type=TodoType(todo_data["type"]),
                tool=todo_data.get("tool"),
                executor=todo_data.get("executor"),
                input_data=todo_data.get("input"),
                arg_template=todo_data.get("arg_template"),
                expected_output=todo_data.get("expected_output", ""),
                needs=todo_data.get("needs", [])
            ))

        state["plan"] = todos
        logger.info(f"Generated plan with {len(todos)} todos")
        for todo in todos:
            logger.info(f"  - {todo.id}: {todo.title} ({todo.type.value})")

    except json.JSONDecodeError as e:
        logger.error(f"JSON parsing failed: {e}")
        logger.error(f"Raw response: {response.content}")
        # Try to extract JSON from text using regex
        json_match = re.search(r'\{.*\}', response.content.strip(), re.DOTALL)
        if json_match:
            try:
                plan_data = json.loads(json_match.group())
                todos = [TodoItem(
                    id=todo_data["id"],
                    title=todo_data["title"],
                    why=todo_data.get("why", ""),
                    type=TodoType(todo_data["type"]),
                    tool=todo_data.get("tool"),
                    input_data=todo_data.get("input"),
                    expected_output=todo_data.get("expected_output", ""),
                    needs=todo_data.get("needs", [])
                ) for todo_data in plan_data.get("todos", [])]
                state["plan"] = todos
                logger.info(f"Recovered plan with {len(todos)} todos from regex")
            except Exception as recovery_error:
                logger.error(f"JSON recovery failed: {recovery_error}")
                state["plan"] = []
        else:
            state["plan"] = []

    except Exception as e:
        logger.error(f"Plan generation failed: {e}")
        # Note: response variable may not be defined if exception occurred during request
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        state["plan"] = []

    state["execution_path"].append("planner_generate")
    state["current_node"] = "planner_generate"

    return state


def planner_gate_node(state: AgentState) -> Dict[str, Any]:
    """
    Gate node that checks if we can proceed with plan execution.
    """
    logger.info("Checking planner gate...")

    # Check if we have missing information
    has_missing_info = any(
        todo.needs for todo in state["plan"]
    ) if state["plan"] else False

    if has_missing_info and state["limits"].max_ask_cycles > 0:
        state["sufficiency"] = "missing"
        state["awaiting_user"] = True
    else:
        state["sufficiency"] = "enough"
        state["awaiting_user"] = False

    state["execution_path"].append("planner_gate")
    state["current_node"] = "planner_gate"

    return state


def resolve_executor(todo: TodoItem) -> str:
    """
    Resolve which executor to use for a todo item.

    Returns:
        "chat" or "reasoner"
    """
    # Priority: todo.executor > TOOL_META.executor_default > "chat"
    if todo.executor and todo.executor != "auto":
        return todo.executor

    # Check tool metadata
    if todo.tool:
        tool_meta = registry.get_tool_meta(todo.tool)
        if tool_meta:
            if tool_meta.executor_default == "reasoner":
                return "reasoner"
            if tool_meta.complexity == "complex":
                return "reasoner"

    # Auto-upgrade conditions for chat â†’ reasoner
    input_data = todo.input_data or {}
    input_str = json.dumps(input_data, ensure_ascii=False)

    # Condition 1: Parameter object too large (>512 chars)
    if len(input_str) > 512:
        logger.info(f"Auto-upgrade to reasoner: large parameter object ({len(input_str)} chars)")
        return "reasoner"

    # Condition 2: Previous validation failures (placeholder for future implementation)
    # This would track validation failures and upgrade after N attempts

    # Condition 3: Complex argument construction needed
    if todo.arg_template:
        logger.info(f"Auto-upgrade to reasoner: argument template required")
        return "reasoner"

    # Condition 4: Complex tool types (placeholder for specific tool types)

    # Default to chat
    return "chat"


def truncate_text(text: str, max_chars: int, suffix: str = "...") -> str:
    """
    Truncate text to max_chars, adding suffix if truncated.

    Args:
        text: Text to truncate
        max_chars: Maximum character count
        suffix: Suffix to add if truncated

    Returns:
        Truncated text
    """
    if len(text) <= max_chars:
        return text
    # Ensure we don't go negative
    keep_chars = max(0, max_chars - len(suffix))
    return text[:keep_chars] + suffix




def call_tool_with_llm(executor: str, tool_name: str, task_context: str, state: AgentState) -> Dict[str, Any]:
    """
    Call a tool using the specified executor with proper two-step protocol.

    Returns:
        Dict with result information
    """
    tool_meta = registry.get_tool_meta(tool_name)
    if not tool_meta:
        return {"success": False, "error": f"Tool {tool_name} not found"}

    # Get appropriate system prompt
    system_prompt = load_system_prompt("chat" if executor == "chat" else "reasoner", executor)

    # For Reasoner executor, provide only the specific tool information
    # For Chat executor, provide full context as before
    if executor == "reasoner":
        # Limit all inputs for Reasoner to prevent empty responses
        task_context = truncate_text(task_context, 200)
        tool_desc = truncate_text(tool_meta.description, 150)
        arg_hint = truncate_text(tool_meta.arg_hint or "", 100)

        # Create focused context for Reasoner - only this specific tool
        context_message = f"""ä»»åŠ¡ï¼š{task_context}

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥å®Œæˆä»»åŠ¡ï¼š
- {tool_name}ï¼š{tool_desc}
  å‚æ•°æç¤ºï¼š{arg_hint}

è¯·è°ƒç”¨ {tool_name} å·¥å…·æ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚åªå…è®¸è°ƒç”¨è¿™ä¸ªå·¥å…·å’Œ ask_user å·¥å…·ï¼ˆå¦‚æœéœ€è¦ç”¨æˆ·ä¿¡æ¯ï¼‰ã€‚
"""
        logger.info(f"Reasoner tool execution: task_context={len(task_context)} chars, "
                   f"tool_desc={len(tool_desc)} chars, arg_hint={len(arg_hint)} chars")
    else:
        # Chat executor gets full context
        tool_desc = tool_meta.description
        arg_hint = tool_meta.arg_hint or ""

        # Create full context message for Chat
        context_message = f"""ä»»åŠ¡ï¼š{task_context}

è¯·è°ƒç”¨ {tool_name} å·¥å…·æ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚
å·¥å…·æè¿°ï¼š{tool_desc}
å‚æ•°æç¤ºï¼š{arg_hint}

åªå…è®¸è°ƒç”¨ {tool_name} å·¥å…·å’Œ ask_user å·¥å…·ï¼ˆå¦‚æœéœ€è¦æ¾„æ¸…ä¿¡æ¯ï¼‰ã€‚
"""

    # Add context to messages
    execution_messages = state["messages"].copy()
    execution_messages.append(Message(role=Role.USER, content=context_message))

    # Create LLM client
    llm_client = create_llm_client(executor)

    retry_count = 0
    max_retries = 2

    while retry_count <= max_retries:
        try:
            # Step 1: Generate tool call
            # For Reasoner, provide only the specific tool function definition
            # For Chat, provide both the tool and ask_user functions
            if executor == "reasoner":
                functions = [{
                    "name": tool_name,
                    "description": tool_desc,  # Use truncated description for Reasoner
                    "parameters": tool_meta.parameters
                }, {
                    "name": "ask_user",
                    "description": "å‘ç”¨æˆ·è¯¢é—®ç¼ºå¤±çš„ä¿¡æ¯",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "question": {"type": "string", "description": "è¦é—®ç”¨æˆ·çš„é—®é¢˜"}
                        },
                        "required": ["question"]
                    }
                }]
            else:
                functions = [{
                    "name": tool_name,
                    "description": tool_meta.description,  # Full description for Chat
                    "parameters": tool_meta.parameters
                }, {
                    "name": "ask_user",
                    "description": "å‘ç”¨æˆ·è¯¢é—®ç¼ºå¤±çš„ä¿¡æ¯",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "question": {"type": "string", "description": "è¦é—®ç”¨æˆ·çš„é—®é¢˜"}
                        },
                        "required": ["question"]
                    }
                }]

            response = llm_client.generate(
                messages=execution_messages,
                functions=functions,
                stream=False
            )

            # Check if we got tool calls
            if not response.tool_calls:
                retry_count += 1
                if retry_count <= max_retries:
                    logger.warning(f"No tool calls generated, retrying ({retry_count}/{max_retries})")
                    continue
                else:
                    return {"success": False, "error": "No tool calls generated after retries"}

            # Process tool calls
            for tool_call in response.tool_calls:
                if tool_call.name == "ask_user":
                    # Need user clarification - this would trigger ask_user_interrupt
                    question = tool_call.arguments.get("question", "éœ€è¦æ›´å¤šä¿¡æ¯")
                    logger.info(f"Tool execution needs clarification: {question}")
                    return {"success": False, "needs_clarification": question}

                elif tool_call.name == tool_name:
                    # Execute the tool
                    try:
                        result = registry.execute(tool_name, **tool_call.arguments)
                        state["tool_call_count"] += 1

                        # Add tool call to conversation
                        assistant_msg = Message(
                            role=Role.ASSISTANT,
                            content="",
                            tool_calls=[ToolCall(
                                id=tool_call.id or f"call_{state['tool_call_count']}",
                                name=tool_call.name,
                                arguments=tool_call.arguments
                            )]
                        )
                        state["messages"].append(assistant_msg)

                        # Add tool response
                        tool_msg = Message(
                            role=Role.TOOL,
                            content=str(result.value) if hasattr(result, 'value') and result.value else str(result.error or "Tool executed"),
                            tool_call_id=tool_call.id or f"call_{state['tool_call_count']}"
                        )
                        state["messages"].append(tool_msg)

                        # Step 2: Generate summary
                        summary_messages = state["messages"].copy()
                        summary_messages.append(Message(
                            role=Role.USER,
                            content="è¯·åŸºäºå·¥å…·æ‰§è¡Œç»“æœï¼Œç»™å‡º1-2å¥ç®€æ´çš„æ€»ç»“ã€‚"
                        ))

                        summary_response = llm_client.generate(
                            messages=summary_messages,
                            stream=False
                        )

                        return {
                            "success": True,
                            "result": result,
                            "summary": summary_response.content,
                            "executor": executor
                        }

                    except Exception as e:
                        logger.error(f"Tool execution failed: {e}")
                        return {"success": False, "error": str(e)}

            # If we get here, no valid tool calls
            return {"success": False, "error": "No valid tool calls found"}

        except Exception as e:
            retry_count += 1
            if retry_count <= max_retries:
                logger.warning(f"Tool call failed, retrying ({retry_count}/{max_retries}): {e}")
            else:
                return {"success": False, "error": f"Tool call failed after retries: {e}"}

    return {"success": False, "error": "Unexpected error in tool execution"}


def todo_dispatch_node(state: AgentState) -> Dict[str, Any]:
    """
    Dispatch the current todo item for execution with executor routing.

    Implements sophisticated executor selection and two-step tool calling protocol.
    """
    logger.info("Dispatching todo item...")

    if not state["plan"]:
        logger.warning("No plan available for dispatch")
        state["should_end"] = True
        state["current_node"] = "end"
        return state

    # Get current todo
    current_idx = state.get("current_todo")
    if current_idx is None:
        current_idx = 0
        state["current_todo"] = 0

    if current_idx >= len(state["plan"]):
        # All todos completed
        state["should_end"] = True
        state["current_node"] = "end"
        return state

    todo = state["plan"][current_idx]

    logger.info(f"Dispatching todo {todo.id}: {todo.title} (type: {todo.type.value})")

    try:
        if todo.type == TodoType.TOOL:
            # Tool and executor should already be decided in planning phase
            if not todo.tool:
                logger.error(f"Tool todo {todo.id} missing tool name from planning phase")
                todo.output = f"é”™è¯¯ï¼šè§„åˆ’é˜¶æ®µæœªæŒ‡å®šå·¥å…·åç§°"
                state["current_todo"] = current_idx + 1
            else:
                # Executor should be set in planning phase, fallback to auto-resolution
                executor = todo.executor if todo.executor and todo.executor != "auto" else resolve_executor(todo)

                # Set up tool call for tool_exec node
                state["pending_tool_call"] = {
                    "tool": todo.tool,
                    "input_data": todo.input_data or {},
                    "todo_id": todo.id,
                    "executor": executor
                }
                logger.info(f"Prepared tool call for {todo.tool} with executor {executor} "
                           f"(decided in planning phase)")
                # Don't advance current_todo yet - wait for tool_exec completion

        elif todo.type == TodoType.CHAT:
            # Use chat model for this todo
            system_prompt = load_system_prompt("chat", "chat")
            additional_context = f"\n\nå½“å‰æ‰§è¡Œä»»åŠ¡ï¼š{todo.title}\næœŸæœ›è¾“å‡ºï¼š{todo.expected_output}"

            llm_client = create_llm_client("chat")
            response = llm_client.generate(
                messages=state["messages"],
                system_prompt=system_prompt + additional_context,
                stream=False
            )

            # Add response to messages
            assistant_msg = Message(role=Role.ASSISTANT, content=response.content)
            state["messages"].append(assistant_msg)

            # Store result in todo
            todo.output = response.content
            logger.info(f"Chat model executed for todo {todo.id}")

        elif todo.type in [TodoType.REASON, TodoType.WRITE, TodoType.RESEARCH]:
            # Use reasoner model for complex tasks
            system_prompt = load_system_prompt("reasoner", "reasoner")
            additional_context = f"\n\nå½“å‰æ‰§è¡Œä»»åŠ¡ï¼š{todo.title}\næœŸæœ›è¾“å‡ºï¼š{todo.expected_output}"

            llm_client = create_llm_client("reasoner")
            response = llm_client.generate(
                messages=state["messages"],
                system_prompt=system_prompt + additional_context,
                stream=False
            )

            # Add response to messages
            assistant_msg = Message(role=Role.ASSISTANT, content=response.content)
            state["messages"].append(assistant_msg)

            # Store result in todo
            todo.output = response.content
            logger.info(f"Reasoner model executed for todo {todo.id}")

        # Check if this todo needs sub-planning
        if todo.type in [TodoType.REASON, TodoType.RESEARCH] and state["depth_budget"] > 0:
            complexity_indicators = ['åˆ†è§£', 'ç»†åŒ–', 'å­ä»»åŠ¡', 'å¤šæ­¥éª¤', 'åˆ†é˜¶æ®µ']
            title_lower = todo.title.lower()
            output_lower = (todo.expected_output or "").lower()
            needs_subplanning = any(
                indicator in title_lower or indicator in output_lower
                for indicator in complexity_indicators
            )
            if needs_subplanning:
                logger.info(f"Todo {todo.id} may benefit from sub-planning (depth_budget: {state['depth_budget']})")
                # Could trigger planner_refine_local here

        # Move to next todo
        state["current_todo"] = current_idx + 1

    except Exception as e:
        logger.error(f"Error executing todo {todo.id}: {e}")
        # Mark todo as failed but continue with next one
        todo.output = f"æ‰§è¡Œå¤±è´¥ï¼š{str(e)}"
        state["current_todo"] = current_idx + 1

    state["execution_path"].append("todo_dispatch")
    state["current_node"] = "todo_dispatch"

    return state


def log_execution_metrics(state: AgentState) -> None:
    """
    Log execution metrics for observability.

    Args:
        state: Current agent state with metrics
    """
    metrics = state.get("metrics", {})

    if not metrics:
        logger.info("No execution metrics recorded")
        return

    # Log micro-reasoning metrics
    micro_calls = metrics.get("reasoner_micro_calls", 0)
    micro_empty = metrics.get("reasoner_micro_empty", 0)
    micro_fallback = metrics.get("reasoner_micro_fallback", 0)

    if micro_calls > 0:
        empty_rate = micro_empty / micro_calls * 100 if micro_calls > 0 else 0
        fallback_rate = micro_fallback / micro_calls * 100 if micro_calls > 0 else 0

        logger.info(f"Micro-reasoning metrics: calls={micro_calls}, empty={micro_empty} ({empty_rate:.1f}%), "
                   f"fallback={micro_fallback} ({fallback_rate:.1f}%)")

    # Log execution summary
    plan_size = len(state.get("plan", []))
    completed_todos = sum(1 for todo in state.get("plan", []) if todo.output)
    tool_calls = state.get("tool_call_count", 0)

    logger.info(f"Execution summary: plan_size={plan_size}, completed={completed_todos}, "
               f"tool_calls={tool_calls}, execution_path={state.get('execution_path', [])}")


def aggregate_answer_node(state: AgentState) -> Dict[str, Any]:
    """
    Aggregate results from all completed todos into final answer.
    """
    logger.info("Aggregating final answer...")

    # Log execution metrics
    log_execution_metrics(state)

    # Collect all assistant responses from the execution
    assistant_responses = [
        msg.content for msg in state["messages"]
        if msg.role.value == "assistant"
    ]

    # Create a comprehensive answer
    if state["plan"]:
        final_answer = f"## æ‰§è¡Œå®Œæˆ\n\n"
        for i, todo in enumerate(state["plan"]):
            final_answer += f"### {todo.title}\n{todo.why}\n\n"

        final_answer += f"## ç»“æœæ±‡æ€»\n\n"
        for i, response in enumerate(assistant_responses[-len(state["plan"]):]):
            final_answer += f"**æ­¥éª¤{i+1}ç»“æœï¼š**\n{response}\n\n"
    else:
        final_answer = "ä»»åŠ¡æ‰§è¡Œå®Œæˆã€‚" + " ".join(assistant_responses[-1:])

    state["final_answer"] = final_answer
    state["should_end"] = True

    state["execution_path"].append("aggregate_answer")
    state["current_node"] = "aggregate_answer"

    return state


