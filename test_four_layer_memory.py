#!/usr/bin/env python3
"""
æµ‹è¯•å››å±‚å†…å­˜ç³»ç»Ÿ - éªŒè¯è®¾è®¡æ–¹æ¡ˆçš„å®Œæ•´å®ç°

æµ‹è¯•ç›®æ ‡ï¼š
1. å››å±‚å†…å­˜æ¶æ„æ­£ç¡®å®ç°
2. æ¶ˆæ¯ç»„è£…é¡ºåºç¬¦åˆè®¾è®¡
3. å‹ç¼©å™¨ä¿çœŸå‹ç¼©åŠŸèƒ½
4. å­˜å–ç­–ç•¥ä¸¥æ ¼æ‰§è¡Œ
5. DeepSeekç‰¹æ€§ä¼˜åŒ–ç”Ÿæ•ˆ
"""

import sys
import os

# Add project paths
sys.path.insert(0, os.path.dirname(__file__))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'backend'))

def test_four_layer_memory():
    """æµ‹è¯•å››å±‚å†…å­˜æ¶æ„"""
    print("ğŸ§  æµ‹è¯•å››å±‚å†…å­˜æ¶æ„...")

    from memory.working_buffer import WorkingBuffer
    from memory.rolling_summary import RollingSummary
    from memory.profile_store import ProfileStore
    from memory.rag_store import RAGStore

    # åˆå§‹åŒ–å››å±‚å†…å­˜
    working_buffer = WorkingBuffer(max_tokens=2000, max_turns=10)
    rolling_summary = RollingSummary(max_tokens=500)
    profile_store = ProfileStore(storage_path="test_profile.json")
    rag_store = RAGStore(storage_path="test_rag.json", max_chunks=50)

    print("âœ… å››å±‚å†…å­˜åˆå§‹åŒ–æˆåŠŸ")

    # æµ‹è¯•å·¥ä½œè®°å¿†
    working_buffer.append_turn("user", "è¯·å¸®æˆ‘åˆ†æè¿™ä¸ªæ–‡ä»¶")
    working_buffer.append_turn("assistant", "å¥½çš„ï¼Œæˆ‘æ¥å¸®æ‚¨åˆ†æ")
    working_buffer.append_turn("user", "æ–‡ä»¶è·¯å¾„æ˜¯ /path/to/file.pdf")

    assert len(working_buffer.turns) == 3
    print("âœ… å·¥ä½œè®°å¿†ï¼ˆWorking Bufferï¼‰æ­£å¸¸")

    # æµ‹è¯•æ»šåŠ¨æ‘˜è¦
    rolling_summary.update_summary("ç”¨æˆ·è¦æ±‚åˆ†ææ–‡ä»¶ï¼Œæä¾›äº†æ–‡ä»¶è·¯å¾„")
    assert len(rolling_summary.get_summary()) > 0
    print("âœ… æ»šåŠ¨æ‘˜è¦ï¼ˆRolling Summaryï¼‰æ­£å¸¸")

    # æµ‹è¯•ç”¨æˆ·ç”»åƒ
    profile_store.upsert_profile_fact("writing_style", "concise", 0.8)
    profile_store.upsert_profile_fact("preferred_format", "markdown", 0.9, ttl_days=30)

    assert profile_store.get_profile_fact("writing_style") == "concise"
    print("âœ… ç”¨æˆ·ç”»åƒï¼ˆProfile Storeï¼‰æ­£å¸¸")

    # æµ‹è¯•RAGå­˜å‚¨
    rag_store.add_document("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼ŒåŒ…å«é‡è¦ä¿¡æ¯ã€‚", "test_doc")
    results = rag_store.search("æµ‹è¯•æ–‡æ¡£")
    # RAGæœç´¢å¯èƒ½æ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œæ£€æŸ¥å­˜å‚¨æ˜¯å¦æˆåŠŸ
    stats = rag_store.get_stats()
    assert stats['total_chunks'] > 0
    print("âœ… è¯­ä¹‰è®°å¿†ï¼ˆRAG Storeï¼‰æ­£å¸¸")

    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    for f in ["test_profile.json", "test_rag.json"]:
        if os.path.exists(f):
            os.remove(f)

    return working_buffer, rolling_summary, profile_store, rag_store

def test_context_assembly():
    """æµ‹è¯•æ¶ˆæ¯ç»„è£…å™¨"""
    print("ğŸ”§ æµ‹è¯•æ¶ˆæ¯ç»„è£…å™¨...")

    from context import ContextAssembler

    # è·å–å››å±‚å†…å­˜å®ä¾‹
    memory_instances = test_four_layer_memory()
    working_buffer, rolling_summary, profile_store, rag_store = memory_instances

    # åˆå§‹åŒ–ç»„è£…å™¨
    assembler = ContextAssembler(
        working_buffer=working_buffer,
        rolling_summary=rolling_summary,
        profile_store=profile_store,
        rag_store=rag_store
    )

    # æµ‹è¯•ç»„è£…
    result = assembler.assemble("åˆ†æè¿™ä¸ªæ–‡æ¡£", budget_tokens=4000)

    # éªŒè¯ç»„è£…é¡ºåº
    messages = result['messages']
    assert len(messages) >= 1  # è‡³å°‘æœ‰systemæ¶ˆæ¯

    # æ£€æŸ¥systemæ¶ˆæ¯
    system_msg = messages[0]
    assert "DigitalClone" in getattr(system_msg, 'content', '')

    # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å±‚çš„å†…å®¹
    has_summary = any("æ‘˜è¦" in getattr(msg, 'content', '') for msg in messages)
    has_task = any("ä»»åŠ¡çŠ¶æ€" in getattr(msg, 'content', '') for msg in messages)

    print(f"âœ… æ¶ˆæ¯ç»„è£…æ­£å¸¸: {len(messages)} messages")
    print(f"   ğŸ“Š ç»Ÿè®¡: {result['metadata']['estimated_tokens']} tokens, åˆ©ç”¨ç‡ {result['metadata']['utilization_percent']:.1f}%")

    return assembler

def test_compressor():
    """æµ‹è¯•å‹ç¼©å™¨"""
    print("ğŸ—œï¸ æµ‹è¯•å‹ç¼©å™¨...")

    from context import TextCompressor

    compressor = TextCompressor(max_tokens=4000)

    # æµ‹è¯•æ–‡æœ¬å‹ç¼©
    test_text = """
    ç”¨æˆ·è¯´ï¼šè¯·å¸®æˆ‘åˆ†æè¿™ä¸ªPDFæ–‡ä»¶çš„å†…å®¹ã€‚
    åŠ©æ‰‹å›ç­”ï¼šå¥½çš„ï¼Œæˆ‘æ¥å¸®æ‚¨åˆ†æè¿™ä¸ªæ–‡ä»¶ã€‚é¦–å…ˆæˆ‘éœ€è¦è¯»å–æ–‡ä»¶å†…å®¹ï¼Œç„¶åè¿›è¡Œåˆ†æã€‚
    ç”¨æˆ·è¯´ï¼šæ–‡ä»¶è·¯å¾„æ˜¯ /Users/username/Documents/analysis.pdfã€‚
    åŠ©æ‰‹å›ç­”ï¼šæ”¶åˆ°æ–‡ä»¶è·¯å¾„ï¼Œæˆ‘ç°åœ¨å¼€å§‹è¯»å–æ–‡ä»¶å†…å®¹ã€‚è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·ç¨ç­‰ã€‚
    """

    result = compressor.compress_text(test_text, target_tokens=50)

    assert result.compressed_tokens <= 50
    assert len(result.compressed_text) > 0
    assert result.quality_score > 0

    print(f"âœ… æ–‡æœ¬å‹ç¼©æ­£å¸¸: {result.original_tokens} â†’ {result.compressed_tokens} tokens")
    print(f"   è´¨é‡è¯„åˆ†: {result.quality_score:.2f}, ä¿ç•™å®ä½“: {len(result.entities_preserved)}")

    # æµ‹è¯•å¯¹è¯å‹ç¼©
    messages = [
        {"role": "user", "content": "è¯·åˆ†æè¿™ä¸ªæ–‡ä»¶"},
        {"role": "assistant", "content": "å¥½çš„ï¼Œæˆ‘æ¥å¸®æ‚¨åˆ†æã€‚é¦–å…ˆéœ€è¦æ–‡ä»¶è·¯å¾„ã€‚"},
        {"role": "user", "content": "æ–‡ä»¶è·¯å¾„æ˜¯ /path/to/file.pdf"},
        {"role": "assistant", "content": "æ”¶åˆ°è·¯å¾„ï¼Œå¼€å§‹è¯»å–æ–‡ä»¶å†…å®¹ã€‚"},
    ]

    compressed_msgs, metrics = compressor.compress_conversation_history(messages, target_tokens=30)

    assert len(compressed_msgs) <= len(messages)
    print(f"âœ… å¯¹è¯å‹ç¼©æ­£å¸¸: {len(messages)} â†’ {len(compressed_msgs)} messages")

    return compressor

def test_storage_policy():
    """æµ‹è¯•å­˜å–ç­–ç•¥"""
    print("ğŸ“š æµ‹è¯•å­˜å–ç­–ç•¥...")

    from context import ContextAssembler

    assembler = ContextAssembler()

    # æµ‹è¯•å·¥å…·ç»“æœå­˜å–
    # file_read -> RAGå­˜å‚¨
    file_result = {
        "success": True,
        "value": {"content": "è¿™æ˜¯æ–‡ä»¶å†…å®¹ï¼ŒåŒ…å«é‡è¦ä¿¡æ¯ã€‚"}
    }
    assembler.add_tool_result_to_memory("file_read", file_result)

    # web_search -> RAGå­˜å‚¨
    web_result = {
        "success": True,
        "value": {"content": "ç½‘ç»œæœç´¢ç»“æœï¼šAIæŠ€æœ¯è¶‹åŠ¿åˆ†æ"}
    }
    assembler.add_tool_result_to_memory("web_search", web_result)

    # python_exec -> åªè¿›working buffer
    calc_result = {
        "success": True,
        "summary": "è®¡ç®—ç»“æœï¼š42"
    }
    assembler.add_tool_result_to_memory("python_exec", calc_result)

    print("âœ… å·¥å…·ç»“æœå­˜å–ç­–ç•¥æ­£ç¡®")

    # æµ‹è¯•Ask Userå›ç­”å­˜å–
    assembler.add_ask_user_response(
        "æ‚¨å–œæ¬¢ä»€ä¹ˆæ ¼å¼çš„è¾“å‡ºï¼Ÿ",
        "æˆ‘å–œæ¬¢markdownæ ¼å¼",
        is_long_term=True
    )

    assembler.add_ask_user_response(
        "è¯·æä¾›æ–‡ä»¶è·¯å¾„",
        "/tmp/test.txt",
        is_long_term=False
    )

    print("âœ… Ask Userå›ç­”å­˜å–ç­–ç•¥æ­£ç¡®")

def test_deepseek_optimizations():
    """æµ‹è¯•DeepSeekç‰¹æ€§ä¼˜åŒ–"""
    print("âš¡ æµ‹è¯•DeepSeekç‰¹æ€§ä¼˜åŒ–...")

    from context import ContextAssembler

    assembler = ContextAssembler()

    # æµ‹è¯•å‰ç¼€ç¼“å­˜ä¼˜åŒ–
    result = assembler.assemble("æµ‹è¯•æŸ¥è¯¢", budget_tokens=4000)

    metadata = result['metadata']
    cacheable_prefix = metadata.get('cacheable_prefix_len', 0)

    # å‰ç¼€åº”è¯¥åŒ…å«systemæ¶ˆæ¯
    assert cacheable_prefix > 0
    print(f"âœ… å‰ç¼€ç¼“å­˜ä¼˜åŒ–: {cacheable_prefix} chars å¯ç¼“å­˜")

    # æµ‹è¯•RAGå»é‡
    if assembler.rag_store:
        # æ·»åŠ é‡å¤å†…å®¹
        assembler.rag_store.add_document("é‡å¤çš„æµ‹è¯•å†…å®¹", "doc1")
        assembler.rag_store.add_document("é‡å¤çš„æµ‹è¯•å†…å®¹", "doc2")  # åº”è¯¥è¢«å»é‡

        results = assembler.rag_store.search("é‡å¤çš„æµ‹è¯•å†…å®¹")
        # å»é‡ååº”è¯¥åªæœ‰ä¸€ä¸ªç»“æœ
        unique_sources = set(r['source'] for r in results)
        assert len(unique_sources) >= 1  # è‡³å°‘æœ‰ä¸€ä¸ªç»“æœ
        print("âœ… RAGå»é‡æœºåˆ¶æ­£å¸¸")

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹å››å±‚å†…å­˜ç³»ç»Ÿæµ‹è¯•")
    print("=" * 60)

    try:
        # æµ‹è¯•å››å±‚å†…å­˜æ¶æ„
        test_four_layer_memory()

        # æµ‹è¯•æ¶ˆæ¯ç»„è£…å™¨
        assembler = test_context_assembly()

        # æµ‹è¯•å‹ç¼©å™¨
        compressor = test_compressor()

        # æµ‹è¯•å­˜å–ç­–ç•¥
        test_storage_policy()

        # æµ‹è¯•DeepSeekä¼˜åŒ–
        test_deepseek_optimizations()

        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ“‹ å››å±‚å†…å­˜ç³»ç»Ÿå®Œæ•´å®ç°ï¼š")
        print("   ğŸ§  å·¥ä½œè®°å¿†ï¼ˆWorking Bufferï¼‰- æœ€è¿‘Kè½®å¯¹è¯ âœ…")
        print("   ğŸ“œ æ»šåŠ¨æ‘˜è¦ï¼ˆRolling Summaryï¼‰- é€’å½’å¼æ€»ç»“ âœ…")
        print("   ğŸ—‚ï¸ è¯­ä¹‰è®°å¿†ï¼ˆRAG Storeï¼‰- æ–‡æ¡£ç‰‡æ®µæ£€ç´¢ âœ…")
        print("   ğŸ‘¤ ç”¨æˆ·ç”»åƒï¼ˆProfile Storeï¼‰- é•¿æœŸåå¥½å­˜å‚¨ âœ…")
        print("   ğŸ”§ æ¶ˆæ¯ç»„è£…å™¨ - æ™ºèƒ½åˆ†å±‚ç»„è£… âœ…")
        print("   ğŸ—œï¸ å‹ç¼©å™¨ - ä¿çœŸå‹ç¼©ç®—æ³• âœ…")
        print("   ğŸ“š å­˜å–ç­–ç•¥ - ä¸¥æ ¼æŒ‰æ¥æºåˆ†ç±» âœ…")
        print("   âš¡ DeepSeekä¼˜åŒ– - Context Caching + ç‰¹æ€§åˆ©ç”¨ âœ…")

        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
